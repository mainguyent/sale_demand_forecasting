{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c83725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'C:\\Users\\maint4\\My Drive\\MCH Sale forecasting\\Works\\data_SO_SI_weekly_by_channel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb038d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a83483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "import isoweek\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# time\n",
    "import datetime as dt\n",
    "import time as time\n",
    "import calendar\n",
    "import holidays\n",
    "# pipeline\n",
    "\n",
    "\n",
    "# models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# optimization and hyperparameters\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "\n",
    "# model evaluation\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error,mean_squared_error\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# saving models\n",
    "from joblib import dump\n",
    "\n",
    "# models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# optimization and hyperparameters\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "\n",
    "# model evaluation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "file_path = r'C:\\Users\\maint4\\My Drive\\MCH Sale forecasting\\Works\\data_SO_SI_weekly_by_channel'\n",
    "#list all the files from the directory\n",
    "file_list = os.listdir(file_path)\n",
    "\n",
    "csv_files = glob.glob('*.{}'.format('csv'))\n",
    "#combine all files in the list\n",
    "df_20_23 = pd.read_csv('bq-results-20231206-110541-1701860782693.csv')\n",
    "inventory_df = pd.read_csv('GT0_Inventory_weekly.csv')\n",
    "#pd.concat([pd.read_csv(f) for f in file_list])\n",
    "Tet_Holiday = pd.read_excel('Master_Date.xlsx', sheet_name= 'Holiday_count')\n",
    "Tet = pd.read_excel('Master_Date.xlsx', sheet_name= 'Tet_Holiday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e227fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_20_23.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ca0120",
   "metadata": {},
   "source": [
    "1. Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63495625",
   "metadata": {},
   "outputs": [],
   "source": [
    "SI_NA = df_20_23[df_20_23[\"STANDARD_SKU_DESC\"].isnull() & df_20_23[\"STANDARD_SKU_DESC_1\"].notnull()]\n",
    "SO_NA = df_20_23[df_20_23[\"STANDARD_SKU_DESC\"].notnull() & df_20_23[\"STANDARD_SKU_DESC_1\"].isnull()]\n",
    "SI_SO = df_20_23[df_20_23[\"STANDARD_SKU_DESC\"].notnull() & df_20_23[\"STANDARD_SKU_DESC_1\"].notnull()]\n",
    "\n",
    "\n",
    "#SO preparation\n",
    "SO_NA['WEEK_NUM'] = SO_NA['WEEK_NUM_SI']\n",
    "\n",
    "print(SO_NA)\n",
    "\n",
    "SO_NA_rm_col =  SO_NA.drop(['WEEK_NUM_SI','WEEK_NUM_SO','YEAR_1','CITY_1','REGION_1','STANDARD_SKU_DESC_1'\n",
    "                            , 'SUB_DIVISION_DESC_1','CATEGORY_DESC_1', 'SUB_CATEGORY_DESC_1','CHANNEL_CODE_1'], axis = 1)\n",
    "\n",
    "SO_NA_rm_col['WEEKLY_SO_ORDER'] = SO_NA_rm_col['WEEKLY_SO_ORDER'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62ae3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SI preparation\n",
    "\n",
    "SI_NA['WEEK_NUM'] = SI_NA['WEEK_NUM_SO']\n",
    "SI_NA['STANDARD_SKU_DESC'] = SI_NA['STANDARD_SKU_DESC_1']\n",
    "SI_NA['YEAR'] = SI_NA['YEAR_1']\n",
    "SI_NA['CHANNEL_CODE'] = SI_NA['CHANNEL_CODE_1']\n",
    "SI_NA['CITY'] = SI_NA['CITY_1']\n",
    "SI_NA['REGION'] = SI_NA['REGION_1']\n",
    "SI_NA['SUB_DIVISION_DESC'] = SI_NA['SUB_DIVISION_DESC_1']\n",
    "SI_NA['CATEGORY_DESC'] = SI_NA['CATEGORY_DESC_1']\n",
    "SI_NA['SUB_CATEGORY_DESC'] = SI_NA['SUB_CATEGORY_DESC_1']\n",
    "\n",
    "SI_NA['WEEKLY_SI_ORDER'] = SI_NA['WEEKLY_SI_ORDER'].fillna(0)\n",
    "\n",
    "\n",
    "SI_NA_rm_col =  SI_NA.drop(['WEEK_NUM_SI','WEEK_NUM_SO','YEAR_1','CITY_1','REGION_1','STANDARD_SKU_DESC_1'\n",
    "                            , 'SUB_DIVISION_DESC_1','CATEGORY_DESC_1', 'SUB_CATEGORY_DESC_1','CHANNEL_CODE_1'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a24954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SI_SO \n",
    "SI_SO['WEEK_NUM'] = SI_SO['WEEK_NUM_SI']\n",
    "SI_SO_rm_col =  SI_SO.drop(['WEEK_NUM_SI','WEEK_NUM_SO','YEAR_1','CITY_1','REGION_1','STANDARD_SKU_DESC_1'\n",
    "                            , 'SUB_DIVISION_DESC_1','CATEGORY_DESC_1', 'SUB_CATEGORY_DESC_1','CHANNEL_CODE_1'], axis = 1)\n",
    "\n",
    "df = pd.concat([SI_SO_rm_col,SO_NA_rm_col,SI_NA_rm_col ], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686b9fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['CHANNEL_CODE'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509583fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "STANDARD_SKU = df[['STANDARD_SKU_DESC','SUB_DIVISION_DESC','CATEGORY_DESC','SUB_CATEGORY_DESC']].drop_duplicates()\n",
    "CITY_REGION = df[['CITY', 'REGION']].drop_duplicates()\n",
    "n_YEAR = df['YEAR'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e75424",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEEK_NUM = []\n",
    "YEAR = []\n",
    "for i in n_YEAR:\n",
    "    for j in range(1,isoweek.Week.last_week_of_year(int(i)).week+1):\n",
    "        YEAR.append(i)\n",
    "        WEEK_NUM.append(j)\n",
    "\n",
    "print(YEAR, WEEK_NUM)\n",
    "\n",
    "list_of_tuples = list(zip(YEAR, WEEK_NUM))\n",
    "\n",
    "WEEK_NUM_YEAR = pd.DataFrame(list_of_tuples,  columns= ['YEAR','WEEK_NUM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d127d943",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(STANDARD_SKU, CITY_REGION, how = 'cross').merge(WEEK_NUM_YEAR, how = 'cross')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949d864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.merge(data,df, how = 'left', on = ['STANDARD_SKU_DESC', 'REGION', 'CITY', 'WEEK_NUM', 'YEAR','SUB_DIVISION_DESC','CATEGORY_DESC','SUB_CATEGORY_DESC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8448ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_full = pd.merge(dataset, inventory_df, how = 'left', on = ['STANDARD_SKU_DESC', 'REGION', 'CITY', 'WEEK_NUM', 'YEAR','SUB_DIVISION_DESC','CATEGORY_DESC','SUB_CATEGORY_DESC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4190ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_full_21_23 = dataset_full[dataset_full['YEAR'] != 2020].drop(['CHANNEL_CODE'], axis = 1)\n",
    "\n",
    "dataset_full_21_23['WEEKLY_SI_ORDER'] = dataset_full_21_23['WEEKLY_SI_ORDER'].fillna(0)\n",
    "dataset_full_21_23['WEEKLY_SO_ORDER'] = dataset_full_21_23['WEEKLY_SO_ORDER'].fillna(0)\n",
    "dataset_full_21_23['LAST_WEEK_INVENTORY'] = dataset_full_21_23['LAST_WEEK_INVENTORY'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebb4ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get weeknumber\n",
    "year_week = dataset_full_21_23['YEAR'].astype(int).astype(str) +'-' + dataset_full_21_23['WEEK_NUM'].astype(int).astype(str)\n",
    "dt.datetime.strptime('2020-49' + '-1', \"%Y-%W-%w\")\n",
    "period_tag = []\n",
    "\n",
    "for i in year_week:\n",
    "    d = dt.datetime.strptime(i + '-1', \"%Y-%W-%w\")\n",
    "    period_tag.append(d)\n",
    "\n",
    "dataset_full_21_23['PERIOD_TAG'] = period_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c652ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "biggest_SKU = dataset_full.copy()\n",
    "biggest_SKU = biggest_SKU.groupby(['STANDARD_SKU_DESC'])[['WEEKLY_SI_ORDER']].sum()\n",
    "biggest_SKU.sort_values(by = ['WEEKLY_SI_ORDER'], ascending = False, inplace = True)\n",
    "biggest_SKU.reset_index(inplace=True)\n",
    "\n",
    "print(biggest_SKU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8536a711",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = dataset_full_21_23.groupby(['STANDARD_SKU_DESC','PERIOD_TAG', 'CITY', 'REGION','YEAR', 'WEEK_NUM', 'SUB_DIVISION_DESC', \n",
    "                  'CATEGORY_DESC', 'SUB_CATEGORY_DESC' ]).agg({'WEEKLY_SI_ORDER' : 'sum', 'WEEKLY_SO_ORDER' : 'sum','LAST_WEEK_INVENTORY' : 'sum' }).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f3cdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['PROVINCE_REGION'] = df2['CITY']+'-'+df2['REGION']\n",
    "df2 = df2.drop(['CITY', 'REGION'], axis = 1)\n",
    "df2['MONTH'] = df2['PERIOD_TAG'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fb0b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add holiday data\n",
    "df2 = df2.merge(Tet_Holiday, how = 'left', left_on=['WEEK_NUM', 'YEAR'], right_on= ['Iso Weeknum','Iso Year'])\n",
    "df2=df2.drop(['Iso Year','Iso Weeknum'], axis= 1) #,'Holiday_count_x','Week_Year_x',\t'Iso Year_y',\t'Iso Weeknum_y',\t'Holiday_count_y',\t'Week_Year_y',\t'Iso Year',\t'Iso Weeknum',\t'Holiday_count',\t'Week_Year'\n",
    "df2.rename(columns = {'Holiday_count' : 'HOLIDAY_COUNT', 'N_Working_day' : 'WORKING_DAY_COUNT'}, inplace = True)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536cfdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separated variable for Tet Holiday occassion\n",
    "df2 = df2.merge(Tet, how = 'left', left_on=['WEEK_NUM', 'YEAR'], right_on= ['Iso Weeknum','Iso Year'])\n",
    "df2=df2.drop(['Iso Year','Iso Weeknum'], axis= 1) #,'Holiday_count_x','Week_Year_x',\t'Iso Year_y',\t'Iso Weeknum_y',\t'Holiday_count_y',\t'Week_Year_y',\t'Iso Year',\t'Iso Weeknum',\t'Holiday_count',\t'Week_Year'\n",
    "df2['IS_TET'] = df2['IS_TET'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf2095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "#seasonality + date variables\n",
    "\n",
    "\n",
    "def is_week(today, week_num):\n",
    "    this_year, this_month = today.year, today.month\n",
    "    weeks = calendar.monthcalendar(this_year, this_month)\n",
    "    return 1 if today.day in weeks[week_num-1] else 0\n",
    "\n",
    "\n",
    "#holiday check\n",
    "def get_holiday_weeks(start_date, end_date, country):\n",
    "    holidays_lis = holidays.CountryHoliday(country)\n",
    "    holiday_dates = [date for date in pd.date_range(start_date, end_date) if date in holidays_lis]\n",
    "    return list(set(str(date.isocalendar().week)+'-'+str(date.isocalendar().year) for date in holiday_dates))\n",
    "    \n",
    "# Calculate week flag\n",
    "df2['IS_2ND_WEEK'] = df2['PERIOD_TAG'].apply(lambda d: is_week(d, 2))\n",
    "df2['IS_3RD_WEEK'] = df2['PERIOD_TAG'].apply(lambda d: is_week(d, 3))\n",
    "df2['IS_4TH_WEEK'] = df2['PERIOD_TAG'].apply(lambda d: is_week(d, 4))\n",
    "\n",
    "# Calculate holiday flags\n",
    "holiday_weeks = get_holiday_weeks('2020-01-01', '2026-01-01', 'VN')\n",
    "df2['IS_HOLIDAY_WEEK'] = df2['PERIOD_TAG'].apply(lambda d: int(str(d.isocalendar().week)+'-'+str(d.isocalendar().year) in holiday_weeks))\n",
    "df2['IS_4W_BF_HOLIDAY'] = df2['PERIOD_TAG'].apply(lambda d: int(str((d + dt.timedelta(weeks=4)).isocalendar().week)+'-'+str((d + dt.timedelta(weeks=4)).isocalendar().year) in holiday_weeks))\n",
    "df2['IS_3W_BF_HOLIDAY'] = df2['PERIOD_TAG'].apply(lambda d: int(str((d + dt.timedelta(weeks=3)).isocalendar().week)+'-'+str((d + dt.timedelta(weeks=3)).isocalendar().year) in holiday_weeks))\n",
    "df2['IS_2W_BF_HOLIDAY'] = df2['PERIOD_TAG'].apply(lambda d: int(str((d + dt.timedelta(weeks=2)).isocalendar().week)+'-'+str((d + dt.timedelta(weeks=2)).isocalendar().year) in holiday_weeks))\n",
    "df2['IS_1W_BF_HOLIDAY'] = df2['PERIOD_TAG'].apply(lambda d: int(str((d + dt.timedelta(weeks=2)).isocalendar().week)+'-'+str((d + dt.timedelta(weeks=1)).isocalendar().year) in holiday_weeks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa420bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#category variables\n",
    "df2['SUB_DIVISION_DESC'] = pd.factorize(df2['SUB_DIVISION_DESC'])[0]\n",
    "df2['CATEGORY_DESC'] = pd.factorize(df2['CATEGORY_DESC'])[0]\n",
    "df2['SUB_CATEGORY_DESC'] = pd.factorize(df2['SUB_CATEGORY_DESC'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2118cb8",
   "metadata": {},
   "source": [
    "2. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7954e9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################3\n",
    "### training and test\n",
    "\n",
    "data_mdl = df2.copy()\n",
    "\n",
    "sellout_w = 13\n",
    "sellin_w = 13\n",
    "target_columns = [f'SO_{horizon}W' for horizon in range(1, sellout_w+1)]+\\\n",
    "                    [f'SI_{horizon}W' for horizon in range(1, sellin_w+1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721ff69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###################################\n",
    "#Pipeline\n",
    "# Select Date as index\n",
    "\n",
    "class Make_Date_Transformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, date_column):\n",
    "        self.date_column = date_column    \n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self    \n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        X_transformed = X.copy()\n",
    "        X_transformed = X_transformed.set_index(self.date_column)\n",
    "        X_transformed.index = pd.to_datetime(X_transformed.index)\n",
    "        return X_transformed  \n",
    "    \n",
    "def model_evaluation(model, Xtrain, ytrain, Xtest, ytest):\n",
    "    def fit_scatter_plot(X, y, set_name):\n",
    "        y_fitted_values = model.predict(X)\n",
    "        xmin = y.min()\n",
    "        xmax = y.max()\n",
    "        plt.scatter(x = y_fitted_values, y = y, alpha=0.25)\n",
    "        x_line = np.linspace(xmin, xmax, 10)\n",
    "        y_line = x_line\n",
    "        plt.plot(x_line, y_line, 'r--')\n",
    "        plt.axhline(0, color=\"black\", linestyle=\"--\")\n",
    "        plt.xlabel('Prediction')\n",
    "        plt.ylabel('True Value')\n",
    "        plt.title(f'Plot of predicted values versus true values - {set_name} set')\n",
    "    \n",
    "    def plot_of_residuals(X, y, set_name):\n",
    "        errors = model.predict(X) - np.reshape(np.array(y), (-1))\n",
    "        plt.scatter(x = y, y = errors, alpha=0.25)\n",
    "        plt.axhline(0, color=\"r\", linestyle=\"--\")\n",
    "        plt.xlabel('True Value')\n",
    "        plt.ylabel('Residual')\n",
    "        plt.title(f'Plot of residuals - {set_name} set')\n",
    "        \n",
    "    def hist_of_residuals(X, y, set_name):\n",
    "        errors = model.predict(X) - np.reshape(np.array(y), (-1))\n",
    "        plt.hist(errors, bins = 100)\n",
    "        plt.axvline(errors.mean(), color='k', linestyle='dashed', linewidth=1)\n",
    "        plt.title(f'Histogram of residuals - {set_name} set')\n",
    "    \n",
    "    def DPA(y_true, y_pred):\n",
    "        dpa = 100 - (((np.sum(np.abs(y_pred - y_true)))/(np.sum(y_true)))*100)\n",
    "        return dpa\n",
    "\n",
    "    def BIAS(y_true, y_pred):\n",
    "        bias = (((np.sum(y_pred - y_true))/(np.sum(y_true)))*100)\n",
    "        return bias\n",
    "    \n",
    "    fig = plt.figure(figsize = (16, 10))\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "    \n",
    "    ax = fig.add_subplot(3, 2, 1)\n",
    "    fit_scatter_plot(X = Xtrain, y = ytrain, set_name = 'train')\n",
    "    \n",
    "    ax = fig.add_subplot(3, 2, 2)\n",
    "    fit_scatter_plot(X = Xtest, y = ytest, set_name = 'test')\n",
    "    \n",
    "    ax = fig.add_subplot(3, 2, 3)\n",
    "    plot_of_residuals(X = Xtrain, y = ytrain, set_name = 'train')\n",
    "    \n",
    "    ax = fig.add_subplot(3, 2, 4)\n",
    "    plot_of_residuals(X = Xtest, y = ytest, set_name = 'test')    \n",
    "\n",
    "    ax = fig.add_subplot(3, 2, 5)\n",
    "    hist_of_residuals(X = Xtrain, y = ytrain, set_name = 'train')   \n",
    "\n",
    "    ax = fig.add_subplot(3, 2, 6)\n",
    "    hist_of_residuals(X = Xtest, y = ytest, set_name = 'test')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    y_pred_train = model.predict(Xtrain)\n",
    "    print(f'RMSE train: {sqrt(mean_squared_error(ytrain, y_pred_train))}')\n",
    "    print(f'DPA  train: {DPA(ytrain, y_pred_train)}')\n",
    "    print(f'BIAS train: {BIAS(ytrain, y_pred_train)}')\n",
    "    print()\n",
    "    y_pred_test = model.predict(Xtest)\n",
    "    print(f'RMSE test:  {sqrt(mean_squared_error(ytest, y_pred_test))}')\n",
    "    print(f'DPA  test: {DPA(ytest, y_pred_test)}')\n",
    "    print(f'BIAS test: {BIAS(ytest, y_pred_test)}')\n",
    "    \n",
    "    # Feature Importance\n",
    "    importance = model._final_estimator.feature_importances_\n",
    "    df_feature_importance=importance.argsort()\n",
    "    df_feature_importance=pd.DataFrame({\n",
    "        'column':Xtrain.columns[df_feature_importance],\n",
    "        'importance':importance[df_feature_importance]\n",
    "    })\n",
    "    df_feature_importance = df_feature_importance[\n",
    "                                            df_feature_importance['importance']>=0.01].copy().reset_index(drop=True)\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    plt.barh(df_feature_importance['column'][-10:], df_feature_importance['importance'][-10:])\n",
    "    plt.tick_params(axis='both', labelsize=10)\n",
    "    plt.title('Model Feature Importance', size=20)\n",
    "    plt.xlabel(' ', size=15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7411861d",
   "metadata": {},
   "source": [
    "2.1 10 highest SI SKUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e573c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store variables\n",
    "SO_test_result = pd.DataFrame()\n",
    "SO_MAPE = []\n",
    "\n",
    "# SI_test_result = pd.DataFrame()\n",
    "# SI_MAPE = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4db58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_test_date = dt.datetime.strptime('2023-03-01', '%Y-%m-%d')\n",
    "end_test_date = dt.datetime.strptime('2023-06-30', '%Y-%m-%d')\n",
    "\n",
    "for i in biggest_SKU.iloc[[0,1,2,4,5,6,7,8,9],0]: #\n",
    "    for j in data_mdl['PROVINCE_REGION'].unique():\n",
    "        print(f'STANDARD_SKU_DESC = {i} and PROVINCE_REGION = {j}')\n",
    "        data_set = data_mdl[(data_mdl['STANDARD_SKU_DESC'] == i) & (data_mdl['PROVINCE_REGION'] == j)\n",
    "                           & (data_mdl['YEAR'] >= 2022)] # \n",
    "\n",
    "        # Select Date as index\n",
    "\n",
    "        data_set = data_set.set_index(data_set['PERIOD_TAG']).dropna()\n",
    "        data_set.index = pd.to_datetime(data_set.index)\n",
    "\n",
    "        historical_vars_window_5 = []\n",
    "        for lag in range(1, 5):\n",
    "\n",
    "            # data_set[f'SI_LAG_{lag}'] = data_set['WEEKLY_SI_ORDER'].shift(lag)\n",
    "            data_set[f'SO_LAG_{lag}'] = data_set['WEEKLY_SO_ORDER'].shift(lag)\n",
    "            historical_vars_window_5.append(f'SO_LAG_{lag}')\n",
    "            if lag >= 2:\n",
    "                # data_set[f'SI_WINDOW_STD_{lag}'] = data_set['WEEKLY_SI_ORDER'].rolling(window=lag).std()\n",
    "                data_set[f'SO_WINDOW_STD_{lag}'] = data_set['WEEKLY_SO_ORDER'].rolling(window=lag).std()\n",
    "                historical_vars_window_5.append(f'SO_WINDOW_STD_{lag}')\n",
    "\n",
    "                # data_set[f'SI_WINDOW_MAX_{lag}'] = data_set['WEEKLY_SI_ORDER'].rolling(window=lag).max()\n",
    "                data_set[f'SO_WINDOW_MAX_{lag}'] = data_set['WEEKLY_SO_ORDER'].rolling(window=lag).max()\n",
    "                historical_vars_window_5.append(f'SO_WINDOW_MAX_{lag}')\n",
    "\n",
    "                # data_set[f'SI_WINDOW_MIN_{lag}'] = data_set['WEEKLY_SI_ORDER'].rolling(window=lag).min()\n",
    "                data_set[f'SO_WINDOW_MIN_{lag}'] = data_set['WEEKLY_SO_ORDER'].rolling(window=lag).min()\n",
    "                historical_vars_window_5.append(f'SO_WINDOW_MIN_{lag}')\n",
    "\n",
    "        # data_set['SI_CUMSUM_BY_WEEK'] = data_set.groupby(['YEAR','MONTH'])['WEEKLY_SI_ORDER'].cumsum()\n",
    "        data_set['SO_CUMSUM_BY_WEEK'] = data_set.groupby(['YEAR', 'MONTH'])['WEEKLY_SO_ORDER'].cumsum()\n",
    "\n",
    "        # data_set['SI_SUM_BY_MONTH'] = data_set.groupby(['YEAR','MONTH'])['WEEKLY_SI_ORDER'].transform(np.sum)\n",
    "        data_set['SO_SUM_BY_MONTH'] = data_set.groupby(['YEAR','MONTH'])['WEEKLY_SO_ORDER'].transform(np.sum)\n",
    "\n",
    "        # data_set['SI_MEAN_BY_MONTH'] = data_set.groupby(['MONTH'])['WEEKLY_SI_ORDER'].transform(lambda x: x.sum()/12)\n",
    "        data_set['SO_MEAN_BY_MONTH'] = data_set.groupby(['MONTH'])['WEEKLY_SO_ORDER'].transform(lambda x: x.sum()/12)\n",
    "\n",
    "        # data_set['SI_MEAN_WINDOW_5'] = data_set['WEEKLY_SI_ORDER'].rolling(window=5).mean()\n",
    "        data_set['SO_MEAN_WINDOW_5'] = data_set['WEEKLY_SO_ORDER'].rolling(window=5).mean()\n",
    "\n",
    "        # data_set['SI_MEAN_WINDOW_10'] = data_set['WEEKLY_SI_ORDER'].rolling(window=10).mean()\n",
    "        data_set['SO_MEAN_WINDOW_10'] = data_set['WEEKLY_SO_ORDER'].rolling(window=10).mean()\n",
    "\n",
    "        data_set[f'SO_WINDOW_STD_10'] = data_set['WEEKLY_SO_ORDER'].rolling(window=10).std()\n",
    "\n",
    "        data_set[f'SO_WINDOW_MAX_10'] = data_set['WEEKLY_SO_ORDER'].rolling(window=10).max()\n",
    "\n",
    "        data_set[f'SO_WINDOW_MIN_10'] = data_set['WEEKLY_SO_ORDER'].rolling(window=10).min()\n",
    "\n",
    "        data_set[f'SO_WINDOW_STD_13'] = data_set['WEEKLY_SO_ORDER'].rolling(window=13).std()\n",
    "\n",
    "        data_set[f'SO_WINDOW_MAX_13'] = data_set['WEEKLY_SO_ORDER'].rolling(window=13).max()\n",
    "\n",
    "        data_set[f'SO_WINDOW_MIN_13'] = data_set['WEEKLY_SO_ORDER'].rolling(window=13).min()\n",
    "\n",
    "        data_set['SO_DIFF_1'] = data_set['WEEKLY_SO_ORDER'].diff()\n",
    "        data_set['SO_DIFF_2'] = data_set['WEEKLY_SO_ORDER'].diff(periods=2)\n",
    "        data_set['SO_DIFF_3'] = data_set['WEEKLY_SO_ORDER'].diff(periods=3)\n",
    "        data_set['SO_DIFF_4'] = data_set['WEEKLY_SO_ORDER'].diff(periods=4)\n",
    "\n",
    "        data_set['SO_MOVING_AVG_WINDOW_4']= (data_set['WEEKLY_SO_ORDER'].rolling(window = 4).sum())/4\n",
    "        data_set['SO_MOVING_AVG_WINDOW_13']= (data_set['WEEKLY_SO_ORDER'].rolling(window = 13).sum())/13\n",
    "\n",
    "        data_set['SO_RESIDUAL_WINDOW_4']= data_set['WEEKLY_SO_ORDER'] - data_set['SO_MOVING_AVG_WINDOW_4']\n",
    "        data_set['SO_RESIDUAL_WINDOW_13']= data_set['WEEKLY_SO_ORDER'] - data_set['SO_MOVING_AVG_WINDOW_13']\n",
    "\n",
    "        data_set['INVENTORY_BEGIN_WEEK'] = data_set['LAST_WEEK_INVENTORY'].shift(1)\n",
    "\n",
    "        #historical vars within window of 5w \n",
    "        historical_vars_window_5.extend(['SO_DIFF_1', 'SO_DIFF_2', 'SO_DIFF_3', 'SO_DIFF_4',  'SO_CUMSUM_BY_WEEK', 'SO_SUM_BY_MONTH', 'SO_MOVING_AVG_WINDOW_4', 'SO_RESIDUAL_WINDOW_4'])\n",
    "        \n",
    "        \n",
    "        #IS_1ST_WEEK\tIS_2ND_WEEK\tIS_3RD_WEEK\tIS_4TH_WEEK\tIS_HOLIDAY_WEEK\tIS_4W_BF_HOLIDAY\tIS_3W_BF_HOLIDAY\tIS_2W_BF_HOLIDAY\n",
    "\n",
    "        analysis = data_set['WEEKLY_SO_ORDER'].copy()\n",
    "\n",
    "        decompose_result_add = seasonal_decompose(analysis, model='additive')\n",
    "            \n",
    "        data_set['SO_SEASONAL'] = decompose_result_add.seasonal\n",
    "        data_set = data_set.drop(['SUB_DIVISION_DESC',\t'CATEGORY_DESC',\t'SUB_CATEGORY_DESC'], axis= 1)\n",
    "\n",
    "        for horizon_prediction in target_columns[0:13]:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            print(f'|---------------------------\\t{horizon_prediction}\\t---------------------------|')\n",
    "            if horizon_prediction[:2] == 'SO':\n",
    "                nr_horizon = int(horizon_prediction[3:-1])\n",
    "                type_prediction = 'Sellout'\n",
    "                target_name = 'Sellout'\n",
    "\n",
    "                #SO training\n",
    "            print('Creating a SellOut training set')\n",
    "            all_set = data_set.copy()\n",
    "\n",
    "            all_set[f'SO_SEASONAL_{nr_horizon}W'] =  all_set['SO_SEASONAL'].shift(-nr_horizon)\n",
    "\n",
    "            all_set[f'WEEK_NUM_{nr_horizon}W'] =  all_set['WEEK_NUM'].shift(-nr_horizon)\n",
    "            all_set[f'HOLIDAY_COUNT_{nr_horizon}W'] =  all_set['HOLIDAY_COUNT'].shift(-nr_horizon)\n",
    "            all_set[f'IS_HOLIDAY_WEEK_{nr_horizon}W'] =  all_set['IS_HOLIDAY_WEEK'].shift(-nr_horizon)\n",
    "            all_set[f'IS_4W_BF_HOLIDAY_{nr_horizon}W'] =  all_set['IS_4W_BF_HOLIDAY'].shift(-nr_horizon)\n",
    "            all_set[f'IS_3W_BF_HOLIDAY_{nr_horizon}W'] =  all_set['IS_3W_BF_HOLIDAY'].shift(-nr_horizon)\n",
    "            all_set[f'IS_2W_BF_HOLIDAY_{nr_horizon}W'] =  all_set['IS_2W_BF_HOLIDAY'].shift(-nr_horizon)\n",
    "            all_set[f'IS_1W_BF_HOLIDAY_{nr_horizon}W'] =  all_set['IS_1W_BF_HOLIDAY'].shift(-nr_horizon)\n",
    "\n",
    "            all_set[horizon_prediction] = all_set['WEEKLY_SO_ORDER'].shift(-nr_horizon).copy()\n",
    "\n",
    "            all_set = all_set.dropna()\n",
    "            if nr_horizon <= 4:\n",
    "\n",
    "                SO_train_set = all_set.loc[(all_set['PERIOD_TAG'] < first_test_date), :]\n",
    "                X_train = SO_train_set.drop(list(all_set.filter(regex='SI_'))+[horizon_prediction]+['PERIOD_TAG','STANDARD_SKU_DESC', 'PROVINCE_REGION','YEAR','MONTH','LAST_WEEK_INVENTORY'], axis = 1).copy()\n",
    "                y_train = SO_train_set[horizon_prediction].copy()\n",
    "                print(X_train.columns)\n",
    "\n",
    "                print('--------------------------------------------------------')\n",
    "\n",
    "                ##### Sell Out Test ##\n",
    "\n",
    "                SO_test_set = all_set.loc[(all_set['PERIOD_TAG'] >= first_test_date)  & \\\n",
    "                                            (all_set['PERIOD_TAG'] <= end_test_date ), :]\n",
    "                X_test =  SO_test_set.drop(list(all_set.filter(regex='SI_'))+[horizon_prediction]+['PERIOD_TAG','STANDARD_SKU_DESC', 'PROVINCE_REGION','YEAR','MONTH','LAST_WEEK_INVENTORY'], axis = 1).copy()\n",
    "                y_test =  SO_test_set[horizon_prediction].copy()\n",
    "            else:\n",
    "                \n",
    "                SO_train_set = all_set.loc[(all_set['PERIOD_TAG'] < first_test_date), :]\n",
    "                X_train = SO_train_set.drop(list(all_set.filter(regex='SI_'))+ historical_vars_window_5 + [horizon_prediction]+['PERIOD_TAG','STANDARD_SKU_DESC', 'PROVINCE_REGION','YEAR','MONTH','LAST_WEEK_INVENTORY'], axis = 1).copy()\n",
    "                y_train = SO_train_set[horizon_prediction].copy()\n",
    "                print(X_train.columns)\n",
    "\n",
    "                print('--------------------------------------------------------')\n",
    "\n",
    "                ##### Sell Out Test ##\n",
    "\n",
    "                SO_test_set = all_set.loc[(all_set['PERIOD_TAG'] >= first_test_date)  & \\\n",
    "                                            (all_set['PERIOD_TAG'] <= end_test_date ), :]\n",
    "                X_test =  SO_test_set.drop(list(all_set.filter(regex='SI_')) + historical_vars_window_5 + [horizon_prediction]+['PERIOD_TAG','STANDARD_SKU_DESC', 'PROVINCE_REGION','YEAR','MONTH','LAST_WEEK_INVENTORY'], axis = 1).copy()\n",
    "                y_test =  SO_test_set[horizon_prediction].copy()\n",
    "\n",
    "\n",
    "            ################################################################ Add a forecast to a dataset ####################\n",
    "            random_forest_pipeline = Pipeline(steps=[('regressor', RandomForestRegressor(random_state=123, n_jobs=3))])\n",
    "\n",
    "            param_random_forest = {\n",
    "                'regressor__max_depth': [5, 10, 15],\n",
    "                'regressor__min_samples_leaf': [3, 7],\n",
    "                'regressor__n_estimators': [100],\n",
    "                'regressor__criterion': ['absolute_error'],\n",
    "            }\n",
    "\n",
    "\n",
    "            if nr_horizon in [1, 2, 3, 4]:\n",
    "                random_forest = GridSearchCV(random_forest_pipeline, param_random_forest, \n",
    "                                            cv=TimeSeriesSplit(n_splits=5).split(X_train), n_jobs=3, \n",
    "                                            scoring = 'neg_mean_absolute_error', verbose=10, error_score = 'raise')\n",
    "                random_forest.fit(X_train, y_train)\n",
    "                # print(\"The best estimator: \", random_forest.best_estimator_,\n",
    "                #     \"\\nThe best score: \", random_forest.best_score_,\n",
    "                #     \"\\nThe best parameters: \", random_forest.best_params_, sep=\"\\n\")\n",
    "\n",
    "                random_forest_model = random_forest.best_estimator_\n",
    "                best_params = random_forest.best_params_\n",
    "            else:\n",
    "                random_forest_model = random_forest_pipeline.set_params(**best_params)\n",
    "                random_forest_model.fit(X_train, y_train)\n",
    "            \n",
    "\n",
    "            SO_test_set[f'Pred_RF_{horizon_prediction}W'] = random_forest_model.predict(X_test)\n",
    "\n",
    "            if nr_horizon in [1, 2, 3, 5, 6, 10, 13]:\n",
    "                print('\\nEvaluation charts')\n",
    "                ## Evaluation check#########################################\n",
    "                model_evaluation(model = random_forest_model,\n",
    "                                Xtrain = X_train, ytrain = y_train,\n",
    "                                Xtest  = X_test,  ytest  = y_test)\n",
    "\n",
    "            RF_SO_MAPE = 1/(len(y_test))*sum(abs(y_test-SO_test_set[f'Pred_RF_{horizon_prediction}W'])/y_test)\n",
    "            \n",
    "            print(f\"Forecast Accuracy RF MAPE: {mean_absolute_percentage_error(y_test, SO_test_set[f'Pred_RF_{horizon_prediction}W'])}\")\n",
    "\n",
    "\n",
    "            plt.figure(figsize=(25,6))\n",
    "            plt.plot(range(0,len(y_test)),y_test, color = 'red', linewidth=2.0, alpha = 0.6)\n",
    "            plt.plot(range(0,len(y_test)), SO_test_set[f'Pred_RF_{horizon_prediction}W'], color = 'blue', linewidth=0.8)\n",
    "            plt.legend(['Actual SO','Predicted SO' ])\n",
    "            plt.xlabel('Timestamp')\n",
    "            plt.title(f\"{i} - {j} - Actual vs prediction\")\n",
    "            plt.show()\n",
    "        \n",
    "\n",
    "\n",
    "        #################################### XGB Model###############################################################################\n",
    "\n",
    "            # matrix_train = xgb.DMatrix(X_train, label = y_train)\n",
    "            # matrix_test = xgb.DMatrix(X_test, label = y_test)\n",
    "\n",
    "            # # Run XGB \n",
    "            # XGBoost\n",
    "#             cv_split = TimeSeriesSplit(n_splits=3, test_size=None)\n",
    "#             model = XGBRegressor()\n",
    "#             parameters = {\n",
    "#                 \"max_depth\": [3, 4, 6, 5, 10],\n",
    "#                 \"learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "#                 \"n_estimators\": [100, 300, 500, 700, 900, 1000],\n",
    "#                 \"colsample_bytree\": [0.3, 0.5, 0.7],\n",
    "#                 \"early_stopping_rounds\": [20]\n",
    "#             }\n",
    "\n",
    "#             XGB = GridSearchCV(estimator=model, cv=cv_split, param_grid=parameters)\n",
    "#             XGB.fit(X_train, y_train)\n",
    "                \n",
    "#             XGB_model = XGB.best_estimator_\n",
    "#             XGB_best_params = XGB.best_params_\n",
    "            \n",
    "            \n",
    "            # xgb_model = xgb.train(params={'objective':'reg:linear','eval_metric':'mape'}\n",
    "            #                 , dtrain = matrix_train, num_boost_round = 500, \n",
    "            #                 early_stopping_rounds = 20, evals = [(matrix_test,'test')],)\n",
    "\n",
    "#             SO_test_set[f'Pred_XGB_{horizon_prediction}W'] = XGB.predict(X_test)\n",
    "\n",
    "            # XGB_SO_MAPE = 1/(len(y_test))*sum(abs((y_test-SO_test_set[f'Pred_XGB_{horizon_prediction}'])/y_test))\n",
    "            \n",
    "            # print(f'Forecast Accuracy XGB MAPE {XGB_SO_MAPE}')\n",
    "            \n",
    "            #SO_MAPE.append([i,j,RF_SO_MAPE]) #,XGB_SO_MAPE\n",
    "\n",
    "#             plt.figure(figsize=(25,6))\n",
    "#             plt.plot(range(0,len(y_test)),y_test, color = 'red', linewidth=2.0, alpha = 0.6)\n",
    "#             plt.plot(range(0,len(y_test)), SO_test_set[f'Pred_XGB_{horizon_prediction}W'], color = 'blue', linewidth=0.8)\n",
    "#             plt.legend(['Actual SO','Predicted SO' ])\n",
    "#             plt.xlabel('Timestamp')\n",
    "#             plt.title(\"Actual vs prediction\")\n",
    "#             plt.show()\n",
    "\n",
    "#             SO_test_result = pd.concat([SO_test_result, SO_test_set])\n",
    "            \n",
    "#             elapsed_time = time.time() - start_time\n",
    "#             print(\"Finished iterating in: \" + str(int(elapsed_time))  + \" seconds\")\n",
    "#             print('\\n')\n",
    "\n",
    "\n",
    "        # #####################################################SI training###################################################################\n",
    "        # print('Creating a Sell In training set')\n",
    "        # SI_train_set = data_set.loc[data_set['PERIOD_TAG'] < first_test_date, :]\n",
    "        # X_train = SI_train_set.drop(list(data_set.filter(regex='^SO'))+['WEEKLY_SI_ORDER','PERIOD_TAG','STANDARD_SKU_DESC', 'PROVINCE_REGION'], axis = 1).copy()\n",
    "        # y_train = SI_train_set['WEEKLY_SI_ORDER'].copy()\n",
    "\n",
    "        # print('--------------------------------------------------------')\n",
    "\n",
    "        # #####%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Sell IN Test %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%#################################\n",
    "        # print('Creating a Sell In test set')\n",
    "\n",
    "        # SI_test_set = data_set.loc[(data_set['PERIOD_TAG'] >= first_test_date) & (data_set['PERIOD_TAG'] <= end_test_date), :]\n",
    "\n",
    "        # X_test =  SI_test_set.drop(list(data_set.filter(regex='^SO'))+['WEEKLY_SI_ORDER','PERIOD_TAG','STANDARD_SKU_DESC', 'PROVINCE_REGION'], axis = 1).copy()\n",
    "        # y_test =  SI_test_set['WEEKLY_SI_ORDER'].copy()\n",
    "\n",
    "        # print('--------------------------------------------------------')\n",
    "\n",
    "        #  ################################################################ Add a forecast to a dataset ####################\n",
    "        # random_forest_pipeline = Pipeline(steps=[('regressor', RandomForestRegressor(random_state=123, n_jobs=3))])\n",
    "\n",
    "        # param_random_forest = {\n",
    "        #     'regressor__max_depth': [5, 10, 15],\n",
    "        #     'regressor__min_samples_leaf': [3, 7],\n",
    "        #     'regressor__n_estimators': [100],\n",
    "        #     'regressor__criterion': ['absolute_error'],\n",
    "        # }\n",
    "\n",
    "        # random_forest = GridSearchCV(random_forest_pipeline, param_random_forest, \n",
    "        #                                  cv=TimeSeriesSplit(n_splits=5).split(X_train), n_jobs=3, \n",
    "        #                                  scoring = 'neg_mean_absolute_error', verbose=10, error_score = 'raise')\n",
    "        # random_forest.fit(X_train, y_train)\n",
    "        # print(\"The best estimator: \", random_forest.best_estimator_,\n",
    "        #       \"\\nThe best score: \", random_forest.best_score_,\n",
    "        #       \"\\nThe best parameters: \", random_forest.best_params_, sep=\"\\n\")\n",
    "\n",
    "        # random_forest_model = random_forest.best_estimator_\n",
    "        # best_params = random_forest.best_params_\n",
    "        \n",
    "        # print('\\nEvaluation charts')\n",
    "\n",
    "        # ### Evaluation check#########################################\n",
    "        # model_evaluation(model = random_forest_model,\n",
    "        #                  Xtrain = X_train, ytrain = y_train,\n",
    "        #                  Xtest  = X_test,  ytest  = y_test)\n",
    "\n",
    "        # SI_test_set['Pred_RF_SI'] = random_forest_model.predict(X_test)\n",
    "\n",
    "        # RF_SI_MAPE = 1/(len(y_test))*sum(abs(y_test-SI_test_set['Pred_RF_SI'])/y_test)\n",
    "\n",
    "        # print(f'Sell In Forecast Accuracy RF MAPE {RF_SI_MAPE}')\n",
    "        \n",
    "        \n",
    "\n",
    "        # plt.figure(figsize=(25,6))\n",
    "        # plt.plot(range(0,len(y_test)),y_test, color = 'red', linewidth=2.0, alpha = 0.6)\n",
    "        # plt.plot(range(0,len(y_test)), SI_test_set['Pred_RF_SI'], color = 'blue', linewidth=0.8)\n",
    "        # plt.legend(['Actual SI','Predicted SI' ])\n",
    "        # plt.xlabel('Timestamp')\n",
    "        # plt.title(\"Actual vs prediction\")\n",
    "        # plt.show()\n",
    "        \n",
    "        \n",
    "        # #################################### XGB Model###############################################################################\n",
    "\n",
    "        # matrix_train = xgb.DMatrix(X_train, label = y_train)\n",
    "        # matrix_test = xgb.DMatrix(X_test, label = y_test)\n",
    "        # # Run XGB \n",
    "        # xgb_model = xgb.train(params={'objective':'reg:linear','eval_metric':'mape'}\n",
    "        #                 , dtrain = matrix_train, num_boost_round = 500, \n",
    "        #                 early_stopping_rounds = 20, evals = [(matrix_test,'test')],)\n",
    "\n",
    "        # SI_test_set['Pred_XGB_SI'] = xgb_model.predict(matrix_test)\n",
    "\n",
    "        # XGB_SI_MAPE = 1/(len(y_test))*sum(abs((y_test-SI_test_set['Pred_XGB_SI'])/y_test))\n",
    "\n",
    "        # SI_test_pre = SI_test_set.copy()\n",
    "        \n",
    "        # SI_test_result = pd.concat([ SI_test_result, SI_test_pre])\n",
    "        \n",
    "        # SI_MAPE.append([i,j,RF_SI_MAPE,XGB_SI_MAPE])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba074a5",
   "metadata": {},
   "source": [
    "2.2 2 Others SKUs in top 100 highest SI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c761a2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store variables\n",
    "SO_test_result_top_100 = pd.DataFrame()\n",
    "# SO_MAPE_top_100 = []\n",
    "\n",
    "# SI_test_result = pd.DataFrame()\n",
    "# SI_MAPE_top_100 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b594357",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_test_date = dt.datetime.strptime('2023-01-01', '%Y-%m-%d')\n",
    "end_test_date = dt.datetime.strptime('2023-03-31', '%Y-%m-%d')\n",
    "\n",
    "for i in biggest_SKU.iloc[10:100,0]:\n",
    "    for j in data_mdl['PROVINCE_REGION'].unique():\n",
    "        print(f'STANDARD_SKU_DESC = {i} and PROVINCE_REGION = {j}')\n",
    "        data_set = data_mdl[(data_mdl['STANDARD_SKU_DESC'] == i) & (data_mdl['PROVINCE_REGION'] == j)\n",
    "                           & (data_mdl['YEAR'] >= 2022)] # \n",
    "        if sum(data_set['WEEKLY_SO_ORDER'] > 0):\n",
    "\n",
    "            # Select Date as index\n",
    "\n",
    "            data_set = data_set.set_index(data_set['PERIOD_TAG']).dropna()\n",
    "            data_set.index = pd.to_datetime(data_set.index)\n",
    "\n",
    "            historical_vars_window_5 = []\n",
    "            for lag in range(1, 5):\n",
    "\n",
    "                # data_set[f'SI_LAG_{lag}'] = data_set['WEEKLY_SI_ORDER'].shift(lag)\n",
    "                data_set[f'SO_LAG_{lag}'] = data_set['WEEKLY_SO_ORDER'].shift(lag)\n",
    "                historical_vars_window_5.append(f'SO_LAG_{lag}')\n",
    "                if lag >= 2:\n",
    "                    # data_set[f'SI_WINDOW_STD_{lag}'] = data_set['WEEKLY_SI_ORDER'].rolling(window=lag).std()\n",
    "                    data_set[f'SO_WINDOW_STD_{lag}'] = data_set['WEEKLY_SO_ORDER'].rolling(window=lag).std()\n",
    "                    historical_vars_window_5.append(f'SO_WINDOW_STD_{lag}')\n",
    "\n",
    "                    # data_set[f'SI_WINDOW_MAX_{lag}'] = data_set['WEEKLY_SI_ORDER'].rolling(window=lag).max()\n",
    "                    data_set[f'SO_WINDOW_MAX_{lag}'] = data_set['WEEKLY_SO_ORDER'].rolling(window=lag).max()\n",
    "                    historical_vars_window_5.append(f'SO_WINDOW_MAX_{lag}')\n",
    "\n",
    "                    # data_set[f'SI_WINDOW_MIN_{lag}'] = data_set['WEEKLY_SI_ORDER'].rolling(window=lag).min()\n",
    "                    data_set[f'SO_WINDOW_MIN_{lag}'] = data_set['WEEKLY_SO_ORDER'].rolling(window=lag).min()\n",
    "                    historical_vars_window_5.append(f'SO_WINDOW_MIN_{lag}')\n",
    "\n",
    "            # data_set['SI_CUMSUM_BY_WEEK'] = data_set.groupby(['YEAR','MONTH'])['WEEKLY_SI_ORDER'].cumsum()\n",
    "            data_set['SO_CUMSUM_BY_WEEK'] = data_set.groupby(['YEAR', 'MONTH'])['WEEKLY_SO_ORDER'].cumsum()\n",
    "\n",
    "            # data_set['SI_SUM_BY_MONTH'] = data_set.groupby(['YEAR','MONTH'])['WEEKLY_SI_ORDER'].transform(np.sum)\n",
    "            data_set['SO_SUM_BY_MONTH'] = data_set.groupby(['YEAR','MONTH'])['WEEKLY_SO_ORDER'].transform(np.sum)\n",
    "\n",
    "            # data_set['SI_MEAN_BY_MONTH'] = data_set.groupby(['MONTH'])['WEEKLY_SI_ORDER'].transform(lambda x: x.sum()/12)\n",
    "            data_set['SO_MEAN_BY_MONTH'] = data_set.groupby(['MONTH'])['WEEKLY_SO_ORDER'].transform(lambda x: x.sum()/12)\n",
    "\n",
    "            # data_set['SI_MEAN_WINDOW_5'] = data_set['WEEKLY_SI_ORDER'].rolling(window=5).mean()\n",
    "            data_set['SO_MEAN_WINDOW_5'] = data_set['WEEKLY_SO_ORDER'].rolling(window=5).mean()\n",
    "\n",
    "            # data_set['SI_MEAN_WINDOW_10'] = data_set['WEEKLY_SI_ORDER'].rolling(window=10).mean()\n",
    "            data_set['SO_MEAN_WINDOW_10'] = data_set['WEEKLY_SO_ORDER'].rolling(window=10).mean()\n",
    "\n",
    "            data_set[f'SO_WINDOW_STD_10'] = data_set['WEEKLY_SO_ORDER'].rolling(window=10).std()\n",
    "\n",
    "            data_set[f'SO_WINDOW_MAX_10'] = data_set['WEEKLY_SO_ORDER'].rolling(window=10).max()\n",
    "\n",
    "            data_set[f'SO_WINDOW_MIN_10'] = data_set['WEEKLY_SO_ORDER'].rolling(window=10).min()\n",
    "\n",
    "            data_set[f'SO_WINDOW_STD_13'] = data_set['WEEKLY_SO_ORDER'].rolling(window=13).std()\n",
    "\n",
    "            data_set[f'SO_WINDOW_MAX_13'] = data_set['WEEKLY_SO_ORDER'].rolling(window=13).max()\n",
    "\n",
    "            data_set[f'SO_WINDOW_MIN_13'] = data_set['WEEKLY_SO_ORDER'].rolling(window=13).min()\n",
    "\n",
    "            data_set['SO_DIFF_1'] = data_set['WEEKLY_SO_ORDER'].diff()\n",
    "            data_set['SO_DIFF_2'] = data_set['WEEKLY_SO_ORDER'].diff(periods=2)\n",
    "            data_set['SO_DIFF_3'] = data_set['WEEKLY_SO_ORDER'].diff(periods=3)\n",
    "            data_set['SO_DIFF_4'] = data_set['WEEKLY_SO_ORDER'].diff(periods=4)\n",
    "\n",
    "            data_set['SO_MOVING_AVG_WINDOW_4']= (data_set['WEEKLY_SO_ORDER'].rolling(window = 4).sum())/4\n",
    "            data_set['SO_MOVING_AVG_WINDOW_13']= (data_set['WEEKLY_SO_ORDER'].rolling(window = 13).sum())/13\n",
    "\n",
    "            data_set['SO_RESIDUAL_WINDOW_4']= data_set['WEEKLY_SO_ORDER'] - data_set['SO_MOVING_AVG_WINDOW_4']\n",
    "            data_set['SO_RESIDUAL_WINDOW_13']= data_set['WEEKLY_SO_ORDER'] - data_set['SO_MOVING_AVG_WINDOW_13']\n",
    "\n",
    "            data_set['INVENTORY_BEGIN_WEEK'] = data_set['LAST_WEEK_INVENTORY'].shift(1)\n",
    "\n",
    "            #historical vars within window of 5w \n",
    "            historical_vars_window_5.extend(['SO_DIFF_1', 'SO_DIFF_2', 'SO_DIFF_3', 'SO_DIFF_4',  'SO_CUMSUM_BY_WEEK', 'SO_SUM_BY_MONTH', 'SO_MOVING_AVG_WINDOW_4', 'SO_RESIDUAL_WINDOW_4'])\n",
    "\n",
    "\n",
    "            #IS_1ST_WEEK\tIS_2ND_WEEK\tIS_3RD_WEEK\tIS_4TH_WEEK\tIS_HOLIDAY_WEEK\tIS_4W_BF_HOLIDAY\tIS_3W_BF_HOLIDAY\tIS_2W_BF_HOLIDAY\n",
    "\n",
    "            analysis = data_set['WEEKLY_SO_ORDER'].copy()\n",
    "\n",
    "            decompose_result_add = seasonal_decompose(analysis, model='additive')\n",
    "\n",
    "            data_set['SO_SEASONAL'] = decompose_result_add.seasonal\n",
    "            data_set = data_set.drop(['SUB_DIVISION_DESC',\t'CATEGORY_DESC',\t'SUB_CATEGORY_DESC'], axis= 1)\n",
    "\n",
    "            for horizon_prediction in target_columns[0:5]:\n",
    "                start_time = time.time()\n",
    "                print(f'|---------------------------\\t{horizon_prediction}\\t---------------------------|')\n",
    "                if horizon_prediction[:2] == 'SO':\n",
    "                    nr_horizon = int(horizon_prediction[3:-1])\n",
    "                    type_prediction = 'Sellout'\n",
    "                    target_name = 'Sellout'\n",
    "\n",
    "                    #SO training\n",
    "                print('Creating a SellOut training set')\n",
    "                all_set = data_set.copy()\n",
    "\n",
    "                all_set[f'SO_SEASONAL_{nr_horizon}W'] =  all_set['SO_SEASONAL'].shift(-nr_horizon)\n",
    "\n",
    "                all_set[f'WEEK_NUM_{nr_horizon}W'] =  all_set['WEEK_NUM'].shift(-nr_horizon)\n",
    "                all_set[f'HOLIDAY_COUNT_{nr_horizon}W'] =  all_set['HOLIDAY_COUNT'].shift(-nr_horizon)\n",
    "                all_set[f'IS_HOLIDAY_WEEK_{nr_horizon}W'] =  all_set['IS_HOLIDAY_WEEK'].shift(-nr_horizon)\n",
    "                all_set[f'IS_4W_BF_HOLIDAY_{nr_horizon}W'] =  all_set['IS_4W_BF_HOLIDAY'].shift(-nr_horizon)\n",
    "                all_set[f'IS_3W_BF_HOLIDAY_{nr_horizon}W'] =  all_set['IS_3W_BF_HOLIDAY'].shift(-nr_horizon)\n",
    "                all_set[f'IS_2W_BF_HOLIDAY_{nr_horizon}W'] =  all_set['IS_2W_BF_HOLIDAY'].shift(-nr_horizon)\n",
    "                all_set[f'IS_1W_BF_HOLIDAY_{nr_horizon}W'] =  all_set['IS_1W_BF_HOLIDAY'].shift(-nr_horizon)\n",
    "\n",
    "                all_set[horizon_prediction] = all_set['WEEKLY_SO_ORDER'].shift(-nr_horizon).copy()\n",
    "\n",
    "                all_set = all_set.dropna()\n",
    "                if nr_horizon <= 4:\n",
    "\n",
    "                    SO_train_set = all_set.loc[(all_set['PERIOD_TAG'] < first_test_date), :]\n",
    "                    X_train = SO_train_set.drop(list(all_set.filter(regex='SI_'))+[horizon_prediction]+['PERIOD_TAG','STANDARD_SKU_DESC', 'PROVINCE_REGION','YEAR','MONTH','LAST_WEEK_INVENTORY'], axis = 1).copy()\n",
    "                    y_train = SO_train_set[horizon_prediction].copy()\n",
    "                    print(X_train.columns)\n",
    "\n",
    "                    print('--------------------------------------------------------')\n",
    "\n",
    "                    ##### Sell Out Test ##\n",
    "\n",
    "                    SO_test_set = all_set.loc[(all_set['PERIOD_TAG'] >= first_test_date)  & \\\n",
    "                                                (all_set['PERIOD_TAG'] <= end_test_date ), :]\n",
    "                    X_test =  SO_test_set.drop(list(all_set.filter(regex='SI_'))+[horizon_prediction]+['PERIOD_TAG','STANDARD_SKU_DESC', 'PROVINCE_REGION','YEAR','MONTH','LAST_WEEK_INVENTORY'], axis = 1).copy()\n",
    "                    y_test =  SO_test_set[horizon_prediction].copy()\n",
    "                else:\n",
    "\n",
    "                    SO_train_set = all_set.loc[(all_set['PERIOD_TAG'] < first_test_date), :]\n",
    "                    X_train = SO_train_set.drop(list(all_set.filter(regex='SI_'))+ historical_vars_window_5 + [horizon_prediction]+['PERIOD_TAG','STANDARD_SKU_DESC', 'PROVINCE_REGION','YEAR','MONTH','LAST_WEEK_INVENTORY'], axis = 1).copy()\n",
    "                    y_train = SO_train_set[horizon_prediction].copy()\n",
    "                    print(X_train.columns)\n",
    "\n",
    "                    print('--------------------------------------------------------')\n",
    "\n",
    "                    ##### Sell Out Test ##\n",
    "\n",
    "                    SO_test_set = all_set.loc[(all_set['PERIOD_TAG'] >= first_test_date)  & \\\n",
    "                                                (all_set['PERIOD_TAG'] <= end_test_date ), :]\n",
    "                    X_test =  SO_test_set.drop(list(all_set.filter(regex='SI_')) + historical_vars_window_5 + [horizon_prediction]+['PERIOD_TAG','STANDARD_SKU_DESC', 'PROVINCE_REGION','YEAR','MONTH','LAST_WEEK_INVENTORY'], axis = 1).copy()\n",
    "                    y_test =  SO_test_set[horizon_prediction].copy()\n",
    "\n",
    "                print('--------------------------------------------------------')\n",
    "\n",
    "                ################################################################ Add a forecast to a dataset ####################\n",
    "                # random_forest_pipeline = Pipeline(steps=[('regressor', RandomForestRegressor(random_state=123, n_jobs=3))])\n",
    "\n",
    "                # param_random_forest = {\n",
    "                #     'regressor__max_depth': [5, 10, 15],\n",
    "                #     'regressor__min_samples_leaf': [3, 7],\n",
    "                #     'regressor__n_estimators': [100],\n",
    "                #     'regressor__criterion': ['absolute_error'],\n",
    "                # }\n",
    "\n",
    "\n",
    "                # # print(\"The best estimator: \", random_forest.best_estimator_,\n",
    "                # #     \"\\nThe best score: \", random_forest.best_score_,\n",
    "                # #     \"\\nThe best parameters: \", random_forest.best_params_, sep=\"\\n\")\n",
    "\n",
    "                # random_forest_model = random_forest.best_estimator_\n",
    "                # best_params = random_forest.best_params_\n",
    "\n",
    "                if nr_horizon in [1, 2, 3, 4, 5]:\n",
    "                    random_forest = GridSearchCV(random_forest_pipeline, param_random_forest, \n",
    "                                                cv=TimeSeriesSplit(n_splits=5).split(X_train), n_jobs=3, \n",
    "                                                scoring = 'neg_mean_absolute_error', verbose=10, error_score = 'raise')\n",
    "                    random_forest.fit(X_train, y_train)\n",
    "                    # print(\"The best estimator: \", random_forest.best_estimator_,\n",
    "                    #     \"\\nThe best score: \", random_forest.best_score_,\n",
    "                    #     \"\\nThe best parameters: \", random_forest.best_params_, sep=\"\\n\")\n",
    "\n",
    "                    random_forest_model = random_forest.best_estimator_\n",
    "                    best_params = random_forest.best_params_\n",
    "                else:\n",
    "                    random_forest_model = random_forest_pipeline.set_params(**best_params)\n",
    "                    random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "                SO_test_set[f'Pred_RF_{horizon_prediction}W'] = random_forest_model.predict(X_test)\n",
    "\n",
    "                if nr_horizon in [1, 2, 3, 6]:\n",
    "                    print('\\nEvaluation charts')\n",
    "                    ## Evaluation check#########################################\n",
    "                    model_evaluation(model = random_forest_model,\n",
    "                                    Xtrain = X_train, ytrain = y_train,\n",
    "                                    Xtest  = X_test,  ytest  = y_test)\n",
    "\n",
    "                RF_SO_MAPE = 1/(len(y_test))*sum(abs(y_test-SO_test_set[f'Pred_RF_{horizon_prediction}W'])/y_test)\n",
    "\n",
    "                print(f'Forecast Accuracy RF MAPE {RF_SO_MAPE}')\n",
    "\n",
    "                plt.figure(figsize=(25,6))\n",
    "                plt.plot(range(0,len(y_test)),y_test, color = 'red', linewidth=2.0, alpha = 0.6)\n",
    "                plt.plot(range(0,len(y_test)), SO_test_set[f'Pred_RF_{horizon_prediction}W'], color = 'blue', linewidth=0.8)\n",
    "                plt.legend(['Actual SO','Predicted SO' ])\n",
    "                plt.xlabel('Timestamp')\n",
    "                plt.title(f\"{i} - {j} - Actual vs prediction\")\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(\"Finished iterating in: \" + str(int(elapsed_time))  + \" seconds\")\n",
    "                print('\\n')\n",
    "            #################################### XGB Model###############################################################################\n",
    "\n",
    "                # matrix_train = xgb.DMatrix(X_train, label = y_train)\n",
    "                # matrix_test = xgb.DMatrix(X_test, label = y_test)\n",
    "\n",
    "                # # Run XGB \n",
    "                # xgb_model = xgb.train(params={'objective':'reg:linear','eval_metric':'mape'}\n",
    "                #                 , dtrain = matrix_train, num_boost_round = 500, \n",
    "                #                 early_stopping_rounds = 20, evals = [(matrix_test,'test')],)\n",
    "\n",
    "                # SO_test_set[f'Pred_XGB_{horizon_prediction}'] = xgb_model.predict(matrix_test)\n",
    "\n",
    "                # XGB_SO_MAPE = 1/(len(y_test))*sum(abs((y_test-SO_test_set[f'Pred_XGB_{horizon_prediction}'])/y_test))\n",
    "\n",
    "                # print(f'Forecast Accuracy XGB MAPE {XGB_SO_MAPE}')\n",
    "\n",
    "                #SO_MAPE.append([i,j,RF_SO_MAPE]) #,XGB_SO_MAPE\n",
    "\n",
    "                # plt.figure(figsize=(25,6))\n",
    "                # plt.plot(range(0,len(y_test)),y_test, color = 'red', linewidth=2.0, alpha = 0.6)\n",
    "                # plt.plot(range(0,len(y_test)), SO_test_set[f'Pred_XGB_{horizon_prediction}'], color = 'blue', linewidth=0.8)\n",
    "                # plt.legend(['Actual SO','Predicted SO' ])\n",
    "                # plt.xlabel('Timestamp')\n",
    "                # plt.title(\"Actual vs prediction\")\n",
    "                # plt.show()\n",
    "\n",
    "                SO_test_result_top_100 = pd.concat([SO_test_result_top_100, SO_test_set])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # #####################################################SI training###################################################################\n",
    "            # print('Creating a Sell In training set')\n",
    "            # SI_train_set = data_set.loc[data_set['PERIOD_TAG'] < first_test_date, :]\n",
    "            # X_train = SI_train_set.drop(list(data_set.filter(regex='^SO'))+['WEEKLY_SI_ORDER','PERIOD_TAG','STANDARD_SKU_DESC', 'PROVINCE_REGION'], axis = 1).copy()\n",
    "            # y_train = SI_train_set['WEEKLY_SI_ORDER'].copy()\n",
    "\n",
    "            # print('--------------------------------------------------------')\n",
    "\n",
    "            # #####%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Sell IN Test %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%#################################\n",
    "            # print('Creating a Sell In test set')\n",
    "\n",
    "            # SI_test_set = data_set.loc[(data_set['PERIOD_TAG'] >= first_test_date) & (data_set['PERIOD_TAG'] <= end_test_date), :]\n",
    "\n",
    "            # X_test =  SI_test_set.drop(list(data_set.filter(regex='^SO'))+['WEEKLY_SI_ORDER','PERIOD_TAG','STANDARD_SKU_DESC', 'PROVINCE_REGION'], axis = 1).copy()\n",
    "            # y_test =  SI_test_set['WEEKLY_SI_ORDER'].copy()\n",
    "\n",
    "            # print('--------------------------------------------------------')\n",
    "\n",
    "            #  ################################################################ Add a forecast to a dataset ####################\n",
    "            # random_forest_pipeline = Pipeline(steps=[('regressor', RandomForestRegressor(random_state=123, n_jobs=3))])\n",
    "\n",
    "            # param_random_forest = {\n",
    "            #     'regressor__max_depth': [5, 10, 15],\n",
    "            #     'regressor__min_samples_leaf': [3, 7],\n",
    "            #     'regressor__n_estimators': [100],\n",
    "            #     'regressor__criterion': ['absolute_error'],\n",
    "            # }\n",
    "\n",
    "            # random_forest = GridSearchCV(random_forest_pipeline, param_random_forest, \n",
    "            #                                  cv=TimeSeriesSplit(n_splits=5).split(X_train), n_jobs=3, \n",
    "            #                                  scoring = 'neg_mean_absolute_error', verbose=10, error_score = 'raise')\n",
    "            # random_forest.fit(X_train, y_train)\n",
    "            # print(\"The best estimator: \", random_forest.best_estimator_,\n",
    "            #       \"\\nThe best score: \", random_forest.best_score_,\n",
    "            #       \"\\nThe best parameters: \", random_forest.best_params_, sep=\"\\n\")\n",
    "\n",
    "            # random_forest_model = random_forest.best_estimator_\n",
    "            # best_params = random_forest.best_params_\n",
    "\n",
    "            # print('\\nEvaluation charts')\n",
    "\n",
    "            # ### Evaluation check#########################################\n",
    "            # model_evaluation(model = random_forest_model,\n",
    "            #                  Xtrain = X_train, ytrain = y_train,\n",
    "            #                  Xtest  = X_test,  ytest  = y_test)\n",
    "\n",
    "            # SI_test_set['Pred_RF_SI'] = random_forest_model.predict(X_test)\n",
    "\n",
    "            # RF_SI_MAPE = 1/(len(y_test))*sum(abs(y_test-SI_test_set['Pred_RF_SI'])/y_test)\n",
    "\n",
    "            # print(f'Sell In Forecast Accuracy RF MAPE {RF_SI_MAPE}')\n",
    "\n",
    "\n",
    "\n",
    "            # plt.figure(figsize=(25,6))\n",
    "            # plt.plot(range(0,len(y_test)),y_test, color = 'red', linewidth=2.0, alpha = 0.6)\n",
    "            # plt.plot(range(0,len(y_test)), SI_test_set['Pred_RF_SI'], color = 'blue', linewidth=0.8)\n",
    "            # plt.legend(['Actual SI','Predicted SI' ])\n",
    "            # plt.xlabel('Timestamp')\n",
    "            # plt.title(\"Actual vs prediction\")\n",
    "            # plt.show()\n",
    "\n",
    "\n",
    "            # #################################### XGB Model###############################################################################\n",
    "\n",
    "            # matrix_train = xgb.DMatrix(X_train, label = y_train)\n",
    "            # matrix_test = xgb.DMatrix(X_test, label = y_test)\n",
    "            # # Run XGB \n",
    "            # xgb_model = xgb.train(params={'objective':'reg:linear','eval_metric':'mape'}\n",
    "            #                 , dtrain = matrix_train, num_boost_round = 500, \n",
    "            #                 early_stopping_rounds = 20, evals = [(matrix_test,'test')],)\n",
    "\n",
    "            # SI_test_set['Pred_XGB_SI'] = xgb_model.predict(matrix_test)\n",
    "\n",
    "            # XGB_SI_MAPE = 1/(len(y_test))*sum(abs((y_test-SI_test_set['Pred_XGB_SI'])/y_test))\n",
    "\n",
    "            # SI_test_pre = SI_test_set.copy()\n",
    "\n",
    "            # SI_test_result = pd.concat([ SI_test_result, SI_test_pre])\n",
    "\n",
    "            # SI_MAPE.append([i,j,RF_SI_MAPE,XGB_SI_MAPE])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
