{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c83725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'C:\\Users\\maint4\\My Drive\\MCH Sale forecasting\\Works\\data_SO_SI_weekly_by_channel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb038d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a83483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "import isoweek\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# time\n",
    "import datetime as dt\n",
    "import time as time\n",
    "import calendar\n",
    "import holidays\n",
    "# pipeline\n",
    "\n",
    "\n",
    "# models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# optimization and hyperparameters\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit, ParameterGrid\n",
    "\n",
    "\n",
    "\n",
    "# model evaluation\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error,mean_squared_error\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# saving models\n",
    "from joblib import dump\n",
    "\n",
    "# models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# optimization and hyperparameters\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "\n",
    "# model evaluation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "file_path = r'C:\\Users\\maint4\\My Drive\\MCH Sale forecasting\\Works\\data_SO_SI_weekly_by_channel'\n",
    "#list all the files from the directory\n",
    "file_list = os.listdir(file_path)\n",
    "\n",
    "csv_files = glob.glob('*.{}'.format('csv'))\n",
    "#combine all files in the list\n",
    "df_20_23 = pd.read_csv('BQ_GT_2301_BYITEMCODE.csv') \n",
    "inventory_df = pd.read_csv('BQ_GT_INVEN_2301_BYITEMCODE.csv')\n",
    "#pd.concat([pd.read_csv(f) for f in file_list])\n",
    "Tet_Holiday = pd.read_excel('Master_Date.xlsx', sheet_name= 'Holiday_count') #local file/can upload to gcp\n",
    "Tet = pd.read_excel('Master_Date.xlsx', sheet_name= 'Tet_Holiday') #local\n",
    "Vegan = pd.read_excel('Master_Date.xlsx', sheet_name= 'Vegan_Week') #local\n",
    "DP_mapping = pd.read_excel('Master_Data.xlsx') #local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a72f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#SO preparation\n",
    "df_20_23['WEEK_NUM'] = df_20_23['WEEK_NUM_SI'].copy()\n",
    "df_20_23['WEEK_NUM'] = df_20_23['WEEK_NUM'].fillna(df_20_23['WEEK_NUM_SO'])\n",
    "\n",
    "df_20_23['ITEM_CODE'] = df_20_23['ITEM_CODE'].fillna(df_20_23['ITEM_CODE_1'])\n",
    "df_20_23['YEAR'] = df_20_23['YEAR'].fillna(df_20_23['YEAR_1'])\n",
    "df_20_23['CITY'] = df_20_23['CITY'].fillna(df_20_23['CITY_1'])\n",
    "\n",
    "df_20_23['WEEKLY_SO_ORDER'] = df_20_23['WEEKLY_SO_ORDER'].fillna(0)\n",
    "df_20_23['WEEKLY_SI_ORDER'] = df_20_23['WEEKLY_SI_ORDER'].fillna(0)\n",
    "\n",
    "df = df_20_23.drop(['WEEK_NUM_SI','WEEK_NUM_SO','YEAR_1','CITY_1','ITEM_CODE_1'], axis = 1)\n",
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e227fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ITEM_CODE'] = df['ITEM_CODE'].str.replace('Z$','',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3185274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v2 = df.merge(DP_mapping[['Product Number', 'Sub Division Name', 'CATEGORY_DESC','SUB_CATEGORY_DESC',\n",
    "                             'Demand Planning Standard SKU Name']], how = 'inner', \n",
    "                 left_on = 'ITEM_CODE', right_on = 'Product Number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0068460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v2.rename(columns = {'Demand Planning Standard SKU Name' : 'DP_STANDARD_SKU_DESC', 'Sub Division Name' : 'SUB_DIVISION_DESC'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70912b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce2c5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_v2[(df_v2['YEAR']==2022) & (df_v2['DP_STANDARD_SKU_DESC']=='Chilli Sauces Chinsu 1kg')]['WEEKLY_SO_ORDER'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbffb3ff",
   "metadata": {},
   "source": [
    "# Impulsing missing weeks in order of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376969e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def week_by_year(n_YEAR):\n",
    "    WEEK_NUM = []\n",
    "    YEAR = []\n",
    "    for i in n_YEAR:\n",
    "        for j in range(1,isoweek.Week.last_week_of_year(int(i)).week+1):\n",
    "            YEAR.append(i)\n",
    "            WEEK_NUM.append(j)\n",
    "    list_of_tuples = list(zip(YEAR, WEEK_NUM))\n",
    "\n",
    "    return pd.DataFrame(list_of_tuples,  columns= ['YEAR','WEEK_NUM'])\n",
    "\n",
    "def make_data_continuous(df):\n",
    "    STANDARD_SKU_CITY = df[['DP_STANDARD_SKU_DESC','SUB_DIVISION_DESC','CATEGORY_DESC','SUB_CATEGORY_DESC','CITY']].drop_duplicates()\n",
    "    n_YEAR = df['YEAR'].unique()\n",
    "    WEEK_NUM_YEAR = week_by_year(n_YEAR)\n",
    "    data = pd.merge(STANDARD_SKU_CITY, WEEK_NUM_YEAR, how = 'cross')\n",
    "    return pd.merge(data,df, how = 'left', on = ['DP_STANDARD_SKU_DESC', 'CITY', 'WEEK_NUM', 'YEAR','SUB_DIVISION_DESC','CATEGORY_DESC','SUB_CATEGORY_DESC'])\n",
    "    \n",
    "def create_period_tag(df):\n",
    "    year_week_1 = df['YEAR'].astype(int).astype(str) +'-' + df['WEEK_NUM'].astype(int).astype(str)\n",
    "    period_tag_1 = []\n",
    "    for i in year_week_1:\n",
    "        d = dt.datetime.strptime(i + '-1', \"%Y-%W-%w\")\n",
    "        period_tag_1.append(d)\n",
    "    df['PERIOD_TAG'] = period_tag_1\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbda045",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = make_data_continuous(df_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949d864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_full = pd.merge(dataset,inventory_df, how = 'left', on = ['ITEM_CODE', 'CITY', 'YEAR', 'WEEK_NUM' ])\n",
    "dataset_full.drop(columns = ['ITEM_CODE','Product Number'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89701e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_full_21_23 = dataset_full[(dataset_full['YEAR'] != 2020)]\n",
    "dataset_full_21_23['WEEKLY_SI_ORDER'] = dataset_full_21_23['WEEKLY_SI_ORDER'].fillna(0)\n",
    "dataset_full_21_23['WEEKLY_SO_ORDER'] = dataset_full_21_23['WEEKLY_SO_ORDER'].fillna(0)\n",
    "dataset_full_21_23['END_WEEK_INVENTORY'] = dataset_full_21_23['END_WEEK_INVENTORY'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3df8f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(dataset_full_21_23[(dataset_full_21_23['YEAR']==2022) & (dataset_full_21_23['DP_STANDARD_SKU_DESC']=='Chilli Sauces Chinsu 1kg')]['WEEKLY_SI_ORDER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebb4ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_full_21_23 = create_period_tag(dataset_full_21_23)\n",
    "df_v2 = create_period_tag(df_v2)\n",
    "dataset_full_21_23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af8f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_full_21_23['STANDARD_SKU_DESC']= dataset_full_21_23['DP_STANDARD_SKU_DESC']+'-'+dataset_full_21_23['SUB_DIVISION_DESC']+'-'+dataset_full_21_23['CATEGORY_DESC']+'-'+dataset_full_21_23['SUB_CATEGORY_DESC']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d370cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v2['STANDARD_SKU_DESC']= df_v2['DP_STANDARD_SKU_DESC']+'-'+df_v2['SUB_DIVISION_DESC']+'-'+df_v2['CATEGORY_DESC']+'-'+df_v2['SUB_CATEGORY_DESC']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8019ab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_record_week = pd.DataFrame(df_v2.groupby(['STANDARD_SKU_DESC','CITY'])['PERIOD_TAG'].agg([np.min, np.max]).reset_index())\n",
    "\n",
    "last_record_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51f4ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_full_21_23 = pd.merge(dataset_full_21_23,last_record_week,how = 'left', on = ['STANDARD_SKU_DESC', 'CITY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_full_21_23=dataset_full_21_23[(dataset_full_21_23['PERIOD_TAG']>= dataset_full_21_23['amin']) \n",
    "                                     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab55bec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(dataset_full_21_23[(dataset_full_21_23['YEAR']==2022) & (dataset_full_21_23['DP_STANDARD_SKU_DESC']=='Chilli Sauces Chinsu 1kg')]['WEEKLY_SI_ORDER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c652ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "biggest_SKU = dataset_full_21_23.copy()\n",
    "biggest_SKU = biggest_SKU.groupby(['STANDARD_SKU_DESC'])[['WEEKLY_SI_ORDER']].sum()\n",
    "biggest_SKU.sort_values(by = ['WEEKLY_SI_ORDER'], ascending = False, inplace = True)\n",
    "biggest_SKU.reset_index(inplace=True)\n",
    "\n",
    "print(biggest_SKU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0493afbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "biggest_SKU.loc[[0,1,2,3,4,5,6,7,8,9,10,11,12],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc1c380",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_full_21_23.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8536a711",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = dataset_full_21_23.groupby(['STANDARD_SKU_DESC','PERIOD_TAG', 'CITY','YEAR', 'WEEK_NUM', 'SUB_DIVISION_DESC', \n",
    "                  'CATEGORY_DESC', 'SUB_CATEGORY_DESC']).agg({'WEEKLY_SI_ORDER' : 'sum', 'WEEKLY_SO_ORDER' : 'sum','END_WEEK_INVENTORY' : 'sum' }).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f4a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df2[(df2['YEAR']==2022) & (df2['STANDARD_SKU_DESC']=='Chilli Sauces Chinsu 1kg-Seasoning-Sauces-Chilli Sauces')]['WEEKLY_SO_ORDER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f3cdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['PROVINCE_REGION'] = df2['CITY'] #+'-'+df2['REGION']\n",
    "df2 = df2.drop(['CITY'], axis = 1)\n",
    "df2['MONTH'] = df2['PERIOD_TAG'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fb0b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.merge(Tet_Holiday, how = 'left', left_on=['WEEK_NUM', 'YEAR'], right_on= ['Iso Weeknum','Iso Year'])\n",
    "df2=df2.drop(['Iso Year','Iso Weeknum'], axis= 1) #,'Holiday_count_x','Week_Year_x',\t'Iso Year_y',\t'Iso Weeknum_y',\t'Holiday_count_y',\t'Week_Year_y',\t'Iso Year',\t'Iso Weeknum',\t'Holiday_count',\t'Week_Year'\n",
    "df2.rename(columns = {'Holiday_count' : 'HOLIDAY_COUNT', 'Working_day' : 'WORKING_DAY_RATE'}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536cfdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.merge(Tet, how = 'left', left_on=['WEEK_NUM', 'YEAR'], right_on= ['Iso Weeknum','Iso Year'])\n",
    "df2=df2.drop(['Iso Year','Iso Weeknum'], axis= 1) #,'Holiday_count_x','Week_Year_x',\t'Iso Year_y',\t'Iso Weeknum_y',\t'Holiday_count_y',\t'Week_Year_y',\t'Iso Year',\t'Iso Weeknum',\t'Holiday_count',\t'Week_Year'\n",
    "df2['IS_TET'] = df2['IS_TET'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e942f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Moon_week = pd.read_excel('Master_Date.xlsx', sheet_name= 'Moon_week')\n",
    "df2 = df2.merge(Moon_week, how = 'left', on=['WEEK_NUM', 'YEAR'])\n",
    "\n",
    "is_1st_day_moon_cal = pd.read_excel('Master_Date.xlsx', sheet_name= '1st_day_moon_cal')\n",
    "df2 = df2.merge(is_1st_day_moon_cal, how = 'left', on=['WEEK_NUM', 'YEAR'])\n",
    "\n",
    "is_15th_day_moon_cal = pd.read_excel('Master_Date.xlsx', sheet_name= '15th_day_moon_cal')\n",
    "df2 = df2.merge(is_15th_day_moon_cal, how = 'left', on=['WEEK_NUM', 'YEAR'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa6721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['IS_1ST_DAY_MOON_CAL'] = df2['IS_1ST_DAY_MOON_CAL'].fillna(0)\n",
    "df2['IS_15TH_DAY_MOON_CAL'] = df2['IS_15TH_DAY_MOON_CAL'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf2095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "#seasonality + date variables\n",
    "\n",
    "\n",
    "def is_week(today, week_num):\n",
    "    this_year, this_month = today.year, today.month\n",
    "    weeks = calendar.monthcalendar(this_year, this_month)\n",
    "    return 1 if today.day in weeks[week_num-1] else 0\n",
    "\n",
    "\n",
    "#holiday check\n",
    "def get_holiday_weeks(start_date, end_date, country):\n",
    "    holidays_lis = holidays.CountryHoliday(country)\n",
    "    holiday_dates = [date for date in pd.date_range(start_date, end_date) if date in holidays_lis]\n",
    "    return list(set(str(date.isocalendar().week)+'-'+str(date.isocalendar().year) for date in holiday_dates))\n",
    "    \n",
    "# Calculate week flag\n",
    "df2['IS_2ND_WEEK'] = df2['PERIOD_TAG'].apply(lambda d: is_week(d, 2))\n",
    "df2['IS_3RD_WEEK'] = df2['PERIOD_TAG'].apply(lambda d: is_week(d, 3))\n",
    "df2['IS_4TH_WEEK'] = df2['PERIOD_TAG'].apply(lambda d: is_week(d, 4))\n",
    "\n",
    "# Calculate holiday flags\n",
    "from isoweek import Week\n",
    "def holiday_check(l_date, u_date, country, df):\n",
    "    WEEK_NUM_YEAR = week_by_year(df['YEAR'].unique())\n",
    "    year_week = WEEK_NUM_YEAR['YEAR'].astype(int).astype(str) +'-' + WEEK_NUM_YEAR['WEEK_NUM'].astype(int).astype(str)\n",
    "    period_tag = []\n",
    "\n",
    "    for i, row in WEEK_NUM_YEAR.iterrows():\n",
    "        d = Week(int(row['YEAR']), int(row['WEEK_NUM'])).thursday()  #dt.datetime.strptime(i + '-1', \"%Y-%W-%w\")\n",
    "        period_tag.append(d)\n",
    "\n",
    "    WEEK_NUM_YEAR['PERIOD_TAG'] = period_tag\n",
    "\n",
    "    holiday_weeks = get_holiday_weeks(l_date, u_date, country)\n",
    "\n",
    "    WEEK_NUM_YEAR['IS_HOLIDAY_WEEK'] = WEEK_NUM_YEAR['PERIOD_TAG'].apply(lambda d: int(str(d.isocalendar().week)+'-'+str(d.isocalendar().year) in holiday_weeks))\n",
    "    # WEEK_NUM_YEAR['IS_13W_BF_HOLIDAY'] = WEEK_NUM_YEAR['PERIOD_TAG'].apply(lambda d: int(str((d + dt.timedelta(weeks=13)).isocalendar().week)+'-'+str((d + dt.timedelta(weeks=13)).isocalendar().year) in holiday_weeks))\n",
    "    # WEEK_NUM_YEAR['IS_12W_BF_HOLIDAY'] = WEEK_NUM_YEAR['PERIOD_TAG'].apply(lambda d: int(str((d + dt.timedelta(weeks=12)).isocalendar().week)+'-'+str((d + dt.timedelta(weeks=12)).isocalendar().year) in holiday_weeks))\n",
    "    # WEEK_NUM_YEAR['IS_11W_BF_HOLIDAY'] = WEEK_NUM_YEAR['PERIOD_TAG'].apply(lambda d: int(str((d + dt.timedelta(weeks=11)).isocalendar().week)+'-'+str((d + dt.timedelta(weeks=11)).isocalendar().year) in holiday_weeks))\n",
    "    WEEK_NUM_YEAR['IS_10W_BF_HOLIDAY'] = WEEK_NUM_YEAR['PERIOD_TAG'].apply(lambda d: int(str((d + dt.timedelta(weeks=10)).isocalendar().week)+'-'+str((d + dt.timedelta(weeks=10)).isocalendar().year) in holiday_weeks))\n",
    "    WEEK_NUM_YEAR['IS_9W_BF_HOLIDAY'] = WEEK_NUM_YEAR['PERIOD_TAG'].apply(lambda d: int(str((d + dt.timedelta(weeks=9)).isocalendar().week)+'-'+str((d + dt.timedelta(weeks=9)).isocalendar().year) in holiday_weeks))\n",
    "    WEEK_NUM_YEAR['IS_8W_BF_HOLIDAY'] = WEEK_NUM_YEAR['PERIOD_TAG'].apply(lambda d: int(str((d + dt.timedelta(weeks=8)).isocalendar().week)+'-'+str((d + dt.timedelta(weeks=8)).isocalendar().year) in holiday_weeks))\n",
    "    WEEK_NUM_YEAR['IS_7W_BF_HOLIDAY'] = WEEK_NUM_YEAR['PERIOD_TAG'].apply(lambda d: int(str((d + dt.timedelta(weeks=7)).isocalendar().week)+'-'+str((d + dt.timedelta(weeks=7)).isocalendar().year) in holiday_weeks))\n",
    "    WEEK_NUM_YEAR['IS_6W_BF_HOLIDAY'] = WEEK_NUM_YEAR['PERIOD_TAG'].apply(lambda d: int(str((d + dt.timedelta(weeks=6)).isocalendar().week)+'-'+str((d + dt.timedelta(weeks=6)).isocalendar().year) in holiday_weeks))\n",
    "    WEEK_NUM_YEAR['IS_5W_BF_HOLIDAY'] = WEEK_NUM_YEAR['PERIOD_TAG'].apply(lambda d: int(str((d + dt.timedelta(weeks=5)).isocalendar().week)+'-'+str((d + dt.timedelta(weeks=5)).isocalendar().year) in holiday_weeks))\n",
    "    WEEK_NUM_YEAR['IS_4W_BF_HOLIDAY'] = WEEK_NUM_YEAR['PERIOD_TAG'].apply(lambda d: int(str((d + dt.timedelta(weeks=4)).isocalendar().week)+'-'+str((d + dt.timedelta(weeks=4)).isocalendar().year) in holiday_weeks))\n",
    "    WEEK_NUM_YEAR['IS_3W_BF_HOLIDAY'] = WEEK_NUM_YEAR['PERIOD_TAG'].apply(lambda d: int(str((d + dt.timedelta(weeks=3)).isocalendar().week)+'-'+str((d + dt.timedelta(weeks=3)).isocalendar().year) in holiday_weeks))\n",
    "    WEEK_NUM_YEAR['IS_2W_BF_HOLIDAY'] = WEEK_NUM_YEAR['PERIOD_TAG'].apply(lambda d: int(str((d + dt.timedelta(weeks=2)).isocalendar().week)+'-'+str((d + dt.timedelta(weeks=2)).isocalendar().year) in holiday_weeks))\n",
    "    WEEK_NUM_YEAR['IS_1W_BF_HOLIDAY'] = WEEK_NUM_YEAR['PERIOD_TAG'].apply(lambda d: int(str((d + dt.timedelta(weeks=1)).isocalendar().week)+'-'+str((d + dt.timedelta(weeks=1)).isocalendar().year) in holiday_weeks))\n",
    "    \n",
    "    return WEEK_NUM_YEAR.drop(['PERIOD_TAG'],axis = 1)\n",
    "\n",
    "# holiday feature\n",
    "holiday_check = holiday_check('2021-01-01', '2026-01-01', 'VN', df2)\n",
    "\n",
    "df2 = df2.merge(holiday_check, how = 'left', on = ['YEAR', 'WEEK_NUM'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4d59d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['WEEK_SIN'] = np.sin((2*np.pi/52)*df2['WEEK_NUM'])\n",
    "df2['WEEK_COS'] = np.cos((2*np.pi/52)*df2['WEEK_NUM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa420bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['SUB_DIVISION_DESC'] = pd.factorize(df2['SUB_DIVISION_DESC'])[0]\n",
    "df2['CATEGORY_DESC'] = pd.factorize(df2['CATEGORY_DESC'])[0]\n",
    "df2['SUB_CATEGORY_DESC'] = pd.factorize(df2['SUB_CATEGORY_DESC'])[0]\n",
    "df2 = df2.merge(Vegan, how = 'left', on = ['YEAR', 'WEEK_NUM'])\n",
    "df2['IS_BIG_VEGAN_WEEK'].fillna(0, inplace = True)\n",
    "df2[\"QUARTER\"] = df2[\"PERIOD_TAG\"].dt.quarter\n",
    "\n",
    "df2[\"QUARTER\"] = df2[\"QUARTER\"].astype('category')\n",
    "df2[\"MONTH\"] = df2[\"MONTH\"].astype('category')\n",
    "df2[\"WEEK_NUM\"] = df2[\"WEEK_NUM\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614ed404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GDP Growth Rate\n",
    "\n",
    "# assign data of lists.  \n",
    "data = {'GDP_quarter': ['2021-01-01','2021-04-01','2021-07-01','2021-10-01',\n",
    "                        '2022-01-01','2022-04-01','2022-07-01','2022-10-01',\n",
    "                        '2023-01-01','2023-04-01','2023-07-01','2023-10-01'\n",
    "                       ], \n",
    "        'GDP_Growth': [4.48, 4.72, 6.73, -6.02,\n",
    "                       5.22, 5.05, 7.83, 13.71, \n",
    "                       5.92, 2.28, 4.14, 4.14]}  \n",
    "  \n",
    "# Create DataFrame  \n",
    "GDP_Data = pd.DataFrame(data)  \n",
    "GDP_Data = GDP_Data.sort_values(['GDP_quarter'], ascending=[True])\n",
    "\n",
    "# Create lagged features\n",
    "for lag in [1,2,3,4]:  # approximate days for 3, 6, 9 and 12 months\n",
    "    GDP_Data[f'gdp_lag_{lag}'] = GDP_Data['GDP_Growth'].shift(lag)\n",
    "\n",
    "GDP_Data = GDP_Data.fillna(0)    \n",
    "\n",
    "\n",
    "    \n",
    "GDP_Data['perc_change_lag1'] =  GDP_Data['GDP_Growth']- (GDP_Data['gdp_lag_1'])/GDP_Data['gdp_lag_1']\n",
    "GDP_Data['perc_change_lag2'] =  GDP_Data['GDP_Growth']- (GDP_Data['gdp_lag_2'])/GDP_Data['gdp_lag_2']\n",
    "GDP_Data['perc_change_lag3'] =  GDP_Data['GDP_Growth']- (GDP_Data['gdp_lag_3'])/GDP_Data['gdp_lag_3']\n",
    "GDP_Data['perc_change_lag4'] =  GDP_Data['GDP_Growth']- (GDP_Data['gdp_lag_4'])/GDP_Data['gdp_lag_4']\n",
    "\n",
    "GDP_Data['perc_change_lag12'] =  (GDP_Data['gdp_lag_1']-GDP_Data['gdp_lag_2'])/GDP_Data['gdp_lag_2']\n",
    "GDP_Data['perc_change_lag23'] =  (GDP_Data['gdp_lag_2']-GDP_Data['gdp_lag_3'])/GDP_Data['gdp_lag_3']\n",
    "GDP_Data['perc_change_lag34'] =  (GDP_Data['gdp_lag_3']-GDP_Data['gdp_lag_4'])/GDP_Data['gdp_lag_4']\n",
    "\n",
    "\n",
    "\n",
    "#GDP_Data[[\"perc_change_lag1\"]] = GDP_Data[[\"perc_change_lag1\"]].fillna(0)\n",
    "\n",
    "\n",
    "cols = [\"perc_change_lag1\",\"perc_change_lag2\",\"perc_change_lag3\",\"perc_change_lag4\",\n",
    "        \"perc_change_lag12\",\"perc_change_lag23\",\"perc_change_lag34\"]\n",
    "\n",
    "for i in range(0,len(cols)):\n",
    "    GDP_Data[cols[i]]= GDP_Data[cols[i]].fillna(0)\n",
    "    GDP_Data[cols[i]]= GDP_Data[cols[i]].replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "\n",
    "df2['calday_quarter'] = pd.PeriodIndex(df2.PERIOD_TAG, freq='q')\n",
    "GDP_Data['gdp_quarter_1'] = pd.PeriodIndex(GDP_Data.GDP_quarter, freq='q')\n",
    "\n",
    "df2 = df2.merge(GDP_Data, left_on ='calday_quarter',right_on='gdp_quarter_1', how='left')\n",
    "\n",
    "col_drop =[\"gdp_quarter_1\",\"calday_quarter\",\"GDP_quarter\"]\n",
    "\n",
    "df2=df2.drop(col_drop, axis=1)\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9762f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CPI Index Vietnam\n",
    "# assign data of lists.  \n",
    "CPI_data = {'CPI_month': ['2021-01-01','2021-02-01','2021-03-01','2021-04-01',\n",
    "                          '2021-05-01','2021-06-01','2021-07-01','2021-08-01',\n",
    "                          '2021-09-01','2021-10-01','2021-11-01','2021-12-01',\n",
    "                          '2022-01-01','2022-02-01','2022-03-01','2022-04-01',\n",
    "                          '2022-05-01','2022-06-01','2022-07-01','2022-08-01',\n",
    "                          '2022-09-01','2022-10-01','2022-11-01','2022-12-01',\n",
    "                          '2023-01-01','2023-02-01','2023-03-01','2023-04-01',\n",
    "                          '2023-05-01','2023-06-01','2023-07-01','2023-08-01',\n",
    "                          '2023-09-01','2023-10-01','2023-11-01','2023-12-01'], \n",
    "            'CPI_index': [ 109.12, 110.87, 109.26, 109.12, 109.16, \n",
    "                          109.02, 109.75, 110.56, 110.38, 108.97, 108.79, 108.97,\n",
    "                          105.28,106.33,107.08,107.28,107.68,108.42,\n",
    "                          108.85,108.86,109.29,109.44,109.87,109.95,\n",
    "                          110.42,110.92,110.67,110.29,110.3,110.59,\n",
    "                           111.09,112.07,112.07,112.07,112.07,112.07]}\n",
    "# Create DataFrame  \n",
    "CPI_Df = pd.DataFrame(CPI_data)\n",
    "CPI_Df = CPI_Df.sort_values(['CPI_index'], ascending=[True])\n",
    "\n",
    "\n",
    "\n",
    "# Create lagged features (# approximate days for 3, 6, 9 and 12 months)\n",
    "for lag in [1,2,3,4]:\n",
    "    CPI_Df[f'cpi_lag_{lag}'] = CPI_Df['CPI_index'].shift(lag)\n",
    "\n",
    "CPI_Df = CPI_Df.fillna(0)\n",
    "\n",
    "\n",
    "CPI_Df['cpi_perc_change_lag1'] = CPI_Df['CPI_index']- (CPI_Df['cpi_lag_1'])/CPI_Df['cpi_lag_1']\n",
    "CPI_Df['cpi_perc_change_lag2'] = CPI_Df['CPI_index']- (CPI_Df['cpi_lag_2'])/CPI_Df['cpi_lag_2']\n",
    "CPI_Df['cpi_perc_change_lag3'] = CPI_Df['CPI_index']- (CPI_Df['cpi_lag_3'])/CPI_Df['cpi_lag_3']\n",
    "CPI_Df['cpi_perc_change_lag4'] = CPI_Df['CPI_index']- (CPI_Df['cpi_lag_4'])/CPI_Df['cpi_lag_4']\n",
    "\n",
    "CPI_Df['cpi_perc_change_lag12'] = (CPI_Df['cpi_lag_1']-CPI_Df['cpi_lag_2'])/CPI_Df['cpi_lag_2']\n",
    "CPI_Df['cpi_perc_change_lag23'] = (CPI_Df['cpi_lag_2']-CPI_Df['cpi_lag_3'])/CPI_Df['cpi_lag_3']\n",
    "CPI_Df['cpi_perc_change_lag34'] = (CPI_Df['cpi_lag_3']-CPI_Df['cpi_lag_4'])/CPI_Df['cpi_lag_4']\n",
    "\n",
    "\n",
    "\n",
    "#GDP_Data[[\"perc_change_lag1\"]] = GDP_Data[[\"perc_change_lag1\"]].fillna(0)\n",
    "\n",
    "\n",
    "cols = [\"cpi_perc_change_lag1\",\"cpi_perc_change_lag2\",\"cpi_perc_change_lag3\",\"cpi_perc_change_lag4\",\n",
    "        \"cpi_perc_change_lag12\",\"cpi_perc_change_lag23\",\"cpi_perc_change_lag34\"]\n",
    "\n",
    "for i in range(0,len(cols)):\n",
    "    CPI_Df[cols[i]]= CPI_Df[cols[i]].fillna(0)\n",
    "    CPI_Df[cols[i]]= CPI_Df[cols[i]].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "\n",
    "df2['data_month'] = (df2['PERIOD_TAG'].dt.floor('d') + pd.offsets.MonthEnd(0) - pd.offsets.MonthBegin(1))\n",
    "\n",
    "\n",
    "df2['data_month']= pd.to_datetime(df2['data_month'])\n",
    "CPI_Df['CPI_month']= pd.to_datetime(CPI_Df['CPI_month'])\n",
    "\n",
    "\n",
    "df2 = df2.merge(CPI_Df, left_on ='data_month',right_on='CPI_month', how='left')\n",
    "\n",
    "\n",
    "\n",
    "col_drop =[\"data_month\",\"CPI_month\"]\n",
    "\n",
    "df2=df2.drop(col_drop, axis=1)\n",
    "#data_final.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc347508",
   "metadata": {},
   "source": [
    "### training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30547251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_mdl['IS_VEGAN_WEEK'].fillna(0, inplace = True)\n",
    "sellout_w = 13\n",
    "sellin_w = 13\n",
    "target_columns = [f'SO_{horizon}W' for horizon in range(1, sellout_w+1)]+\\\n",
    "                    [f'SI_{horizon}W' for horizon in range(1, sellin_w+1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721ff69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###################################\n",
    "#Pipeline\n",
    "# Select Date as index\n",
    "\n",
    "class Make_Date_Transformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, date_column):\n",
    "        self.date_column = date_column    \n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self    \n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        X_transformed = X.copy()\n",
    "        X_transformed = X_transformed.set_index(self.date_column)\n",
    "        X_transformed.index = pd.to_datetime(X_transformed.index)\n",
    "        return X_transformed  \n",
    "    \n",
    "def model_evaluation(model, Xtrain, ytrain, Xtest, ytest):\n",
    "    def fit_scatter_plot(X, y, set_name):\n",
    "        y_fitted_values = model.predict(X)\n",
    "        xmin = y.min()\n",
    "        xmax = y.max()\n",
    "        plt.scatter(x = y_fitted_values, y = y, alpha=0.25)\n",
    "        x_line = np.linspace(xmin, xmax, 10)\n",
    "        y_line = x_line\n",
    "        plt.plot(x_line, y_line, 'r--')\n",
    "        plt.axhline(0, color=\"black\", linestyle=\"--\")\n",
    "        plt.xlabel('Prediction')\n",
    "        plt.ylabel('True Value')\n",
    "        plt.title(f'Plot of predicted values versus true values - {set_name} set')\n",
    "    \n",
    "    def plot_of_residuals(X, y, set_name):\n",
    "        errors = model.predict(X) - np.reshape(np.array(y), (-1))\n",
    "        plt.scatter(x = y, y = errors, alpha=0.25)\n",
    "        plt.axhline(0, color=\"r\", linestyle=\"--\")\n",
    "        plt.xlabel('True Value')\n",
    "        plt.ylabel('Residual')\n",
    "        plt.title(f'Plot of residuals - {set_name} set')\n",
    "        \n",
    "    def hist_of_residuals(X, y, set_name):\n",
    "        errors = model.predict(X) - np.reshape(np.array(y), (-1))\n",
    "        plt.hist(errors, bins = 100)\n",
    "        plt.axvline(errors.mean(), color='k', linestyle='dashed', linewidth=1)\n",
    "        plt.title(f'Histogram of residuals - {set_name} set')\n",
    "    \n",
    "    def DPA(y_true, y_pred):\n",
    "        dpa = 100 - (((np.sum(np.abs(y_pred - y_true)))/(np.sum(y_true)))*100)\n",
    "        return dpa\n",
    "\n",
    "    def BIAS(y_true, y_pred):\n",
    "        bias = (((np.sum(y_pred - y_true))/(np.sum(y_true)))*100)\n",
    "        return bias\n",
    "    \n",
    "    fig = plt.figure(figsize = (16, 10))\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "    \n",
    "    ax = fig.add_subplot(3, 2, 1)\n",
    "    fit_scatter_plot(X = Xtrain, y = ytrain, set_name = 'train')\n",
    "    \n",
    "    ax = fig.add_subplot(3, 2, 2)\n",
    "    fit_scatter_plot(X = Xtest, y = ytest, set_name = 'test')\n",
    "    \n",
    "    ax = fig.add_subplot(3, 2, 3)\n",
    "    plot_of_residuals(X = Xtrain, y = ytrain, set_name = 'train')\n",
    "    \n",
    "    ax = fig.add_subplot(3, 2, 4)\n",
    "    plot_of_residuals(X = Xtest, y = ytest, set_name = 'test')    \n",
    "\n",
    "    ax = fig.add_subplot(3, 2, 5)\n",
    "    hist_of_residuals(X = Xtrain, y = ytrain, set_name = 'train')   \n",
    "\n",
    "    ax = fig.add_subplot(3, 2, 6)\n",
    "    hist_of_residuals(X = Xtest, y = ytest, set_name = 'test')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    y_pred_train = model.predict(Xtrain)\n",
    "    print(f'RMSE train: {sqrt(mean_squared_error(ytrain, y_pred_train))}')\n",
    "    print(f'DPA  train: {DPA(ytrain, y_pred_train)}')\n",
    "    print(f'BIAS train: {BIAS(ytrain, y_pred_train)}')\n",
    "    print()\n",
    "    y_pred_test = model.predict(Xtest)\n",
    "    print(f'RMSE test:  {sqrt(mean_squared_error(ytest, y_pred_test))}')\n",
    "    print(f'DPA  test: {DPA(ytest, y_pred_test)}')\n",
    "    print(f'BIAS test: {BIAS(ytest, y_pred_test)}')\n",
    "    \n",
    "    # Feature Importance\n",
    "    importance = model._final_estimator.feature_importances_\n",
    "    df_feature_importance=importance.argsort()\n",
    "    df_feature_importance=pd.DataFrame({\n",
    "        'column':Xtrain.columns[df_feature_importance],\n",
    "        'importance':importance[df_feature_importance]\n",
    "    })\n",
    "    df_feature_importance = df_feature_importance[\n",
    "                                            df_feature_importance['importance']>=0.01].copy().reset_index(drop=True)\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    plt.barh(df_feature_importance['column'][-10:], df_feature_importance['importance'][-10:])\n",
    "    plt.tick_params(axis='both', labelsize=10)\n",
    "    plt.title('Model Feature Importance', size=20)\n",
    "    plt.xlabel(' ', size=15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43c11be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform training and testset\n",
    "\n",
    "def create_sale_lag(df):\n",
    "    for lag in range(1, 13):\n",
    "        df[f'SO_LAG_{lag}'] = df['WEEKLY_SO_ORDER'].shift(lag)\n",
    "    return df\n",
    "\n",
    "def create_growth_lag(df):\n",
    "    for lag in range(1, 13):\n",
    "        df[f'SO_GROWTH_{lag}'] = df['SO_GROWTH'].shift(lag)\n",
    "        df[f'SO_GROWTH_{lag}'].replace([np.inf,-np.inf],100,inplace=True)\n",
    "    return df\n",
    "\n",
    "def create_sale_expanding(df):\n",
    "    for lag in range(1, 14):\n",
    "        df[f'SO_EXPANDING_MEAN_{lag}'] = df['WEEKLY_SO_ORDER'].expanding(lag).mean()\n",
    "        df[f'SO_EXPANDING_STD_{lag}'] = df['WEEKLY_SO_ORDER'].expanding(lag).std()\n",
    "        df[f'SO_EXPANDING_MAX_{lag}'] = df['WEEKLY_SO_ORDER'].expanding(lag).max()\n",
    "        df[f'SO_EXPANDING_MIN_{lag}'] = df['WEEKLY_SO_ORDER'].expanding(lag).min()\n",
    "        \n",
    "def create_sale_rolling(df):\n",
    "    for lag in range(1, 14):\n",
    "        df[f'SO_WINDOW_MEAN_{lag}'] = data_set['WEEKLY_SO_ORDER'].rolling(window=lag).mean()\n",
    "        if lag >= 2:\n",
    "            df[f'SO_WINDOW_STD_{lag}'] = df['WEEKLY_SO_ORDER'].rolling(window=lag).std()\n",
    "            df[f'SO_WINDOW_MAX_{lag}'] = df['WEEKLY_SO_ORDER'].rolling(window=lag).max()\n",
    "            df[f'SO_WINDOW_MIN_{lag}'] = df['WEEKLY_SO_ORDER'].rolling(window=lag).min()\n",
    "            \n",
    "def create_sale_ewma(df):\n",
    "    for lag in range(2, 14):\n",
    "        df[f'SO_EWMA_MEAN_{lag}'] = df['WEEKLY_SO_ORDER'].ewm(span=lag).mean()\n",
    "        df[f'SO_EWMA_STD_{lag}'] = df['WEEKLY_SO_ORDER'].ewm(span=lag).std()  \n",
    "\n",
    "def create_seasonality_index(df,base_year):\n",
    "    avg = df[df['YEAR'] == base_year]['WEEKLY_SO_ORDER'].mean()\n",
    "    df['SEASONALITY_INDEX'] = df['WEEKLY_SO_ORDER']/avg\n",
    "    for lag in range(1,13):\n",
    "        df[f'SEASONALITY_INDEX_LAG_{lag}'] = df['SEASONALITY_INDEX'].shift(lag)\n",
    "\n",
    "def mdl_transform(SKU_name,province,df):\n",
    "    data_set = df[(df['STANDARD_SKU_DESC'] == SKU_name) & (df['PROVINCE_REGION'] == province)] \n",
    "    \n",
    "    data_set = data_set.set_index(data_set['PERIOD_TAG']).dropna()\n",
    "    data_set.index = pd.to_datetime(data_set.index)\n",
    "\n",
    "    if (SKU_name == 'Instant Noodle Kokomi Tôm 30gói 65gr-Convenience Foods-Noodles-Instant Noodle') & (province == 'T. Bắc Giang') | (j == 'T. Hải Dương'):\n",
    "        data_set['WEEKLY_SO_ORDER'] = data_set['WEEKLY_SO_ORDER'].replace(0.0,np.nan).fillna(data_set['WEEKLY_SI_ORDER'])\n",
    "    \n",
    "    #growth feature\n",
    "    data_set['SO_LAG_1'] = data_set['WEEKLY_SO_ORDER'].shift(1)\n",
    "    data_set['SO_LAG_1'].fillna(0.001,inplace=True)\n",
    "\n",
    "    data_set['SO_GROWTH'] = (data_set['WEEKLY_SO_ORDER']/data_set['SO_LAG_1'] - 1)*100\n",
    "    data_set['SO_GROWTH'].replace([np.inf,-np.inf],100,inplace=True)\n",
    "    data_set['SO_GROWTH'].fillna(0, inplace = True)\n",
    "    \n",
    "    data_set = create_growth_lag(data_set)\n",
    "    data_set = create_sale_lag(data_set)\n",
    "    data_set = create_sale_expanding(data_set)\n",
    "    data_set = create_sale_rolling(data_set)\n",
    "    data_set = create_sale_ewma(data_set)\n",
    "    data_set = create_seasonality_index(data_set, 2022)\n",
    "    \n",
    "    #data_set['SI_CUMSUM_BY_WEEK'] = data_set.groupby(['YEAR','MONTH'])['WEEKLY_SI_ORDER'].cumsum()\n",
    "#             data_set['SO_CUMSUM_BY_WEEK'] = data_set.groupby(['YEAR', 'MONTH'])['WEEKLY_SO_ORDER'].cumsum()\n",
    "#             data_set['SO_CUMSUM_BY_QUARTER'] = data_set.groupby(['YEAR','QUARTER'])['WEEKLY_SO_ORDER'].cumsum()\n",
    "\n",
    "    data_set['SO_MEAN_BY_MONTH'] = data_set.groupby(['MONTH'])['WEEKLY_SO_ORDER'].transform(lambda x: x.sum()/data_set['YEAR'].nunique())\n",
    "\n",
    "#             data_set['INVENTORY_BEGIN_WEEK'] = data_set['END_WEEK_INVENTORY'].shift(1)\n",
    "#             data_set['STOCK_AVILABILITY'] = data_set['WEEKLY_SI_ORDER'] + data_set['INVENTORY_BEGIN_WEEK']\n",
    "\n",
    "    for col in data_set._get_numeric_data().columns.values:\n",
    "        data_set[col].fillna(data_set[col].mean(), inplace=True)\n",
    "    \n",
    "    data_set.replace([np.inf,-np.inf],100,inplace=True)\n",
    "#             data_set['SO_DIFF'] = data_set['WEEKLY_SO_ORDER'] - data_set['SO_LAG_1']\n",
    "\n",
    "    data_set = data_set.drop(['SUB_DIVISION_DESC',\t'CATEGORY_DESC',\t'SUB_CATEGORY_DESC'], axis= 1)\n",
    "    return data_set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772b727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SO_test_result = pd.DataFrame()\n",
    "SO_Pred = pd.DataFrame()\n",
    "SO_test_result_ETS = pd.DataFrame()\n",
    "final_features_top = []\n",
    "count = 0\n",
    "\n",
    "first_test_date = dt.datetime.strptime('2023-06-30', '%Y-%m-%d')\n",
    "end_test_date = dt.datetime.strptime('2023-09-30', '%Y-%m-%d')\n",
    "\n",
    "for i in biggest_SKU.iloc[20:30,0]: #4,8 #0,1,2,3,4,5,6,\n",
    "    for j in df2[(data_mdl['STANDARD_SKU_DESC'] == i)]['PROVINCE_REGION'].unique(): #\n",
    "        print(f'STANDARD_SKU_DESC = {i} and PROVINCE_REGION = {j}')\n",
    "        if (\n",
    "            (sum(data_set[data_set['YEAR']==2023]['WEEKLY_SO_ORDER']) > 0) & \\\n",
    "            (sum(data_set[data_set['YEAR']==2023]['WEEKLY_SI_ORDER']) > 0) & \\\n",
    "            (data_set[(data_set['YEAR']>=2022)&(data_set['PERIOD_TAG'] < first_test_date)].shape[0] >= 52)):\n",
    "\n",
    "            data_set = mdl_transform(i,j,df2)\n",
    "\n",
    "            for horizon_prediction in target_columns[0:13]:\n",
    "                start_time = time.time()\n",
    "\n",
    "                print(f'|---------------------------\\t{horizon_prediction}\\t---------------------------|')\n",
    "                if horizon_prediction[:2] == 'SO':\n",
    "                    nr_horizon = int(horizon_prediction[3:-1])\n",
    "                    type_prediction = 'Sellout'\n",
    "                    target_name = 'Sellout'\n",
    "\n",
    "                    #SO training\n",
    "                print('Creating a SellOut training set')\n",
    "                all_set = data_set[(data_set['YEAR'] >= 2022)].copy()\n",
    "                all_set[f'WEEK_NUM_{nr_horizon}W'] =  all_set['WEEK_NUM'].shift(-nr_horizon)\n",
    "#                     all_set[f'MOON_WEEK_{nr_horizon}W'] =  all_set['MOON_WEEK'].shift(-nr_horizon)\n",
    "#                     all_set[f'IS_1ST_DAY_MOON_CAL_{nr_horizon}W'] =  all_set['IS_1ST_DAY_MOON_CAL'].shift(-nr_horizon)\n",
    "#                     all_set[f'IS_15TH_DAY_MOON_CAL_{nr_horizon}W'] =  all_set['IS_15TH_DAY_MOON_CAL'].shift(-nr_horizon)\n",
    "\n",
    "                all_set[f'WEEK_SIN_{nr_horizon}W'] =  all_set['WEEK_SIN'].shift(-nr_horizon)\n",
    "                all_set[f'WEEK_COS_{nr_horizon}W'] =  all_set['WEEK_COS'].shift(-nr_horizon)\n",
    "                all_set[f'HOLIDAY_COUNT_{nr_horizon}W'] =  all_set['HOLIDAY_COUNT'].shift(-nr_horizon)\n",
    "                all_set[f'IS_HOLIDAY_WEEK_{nr_horizon}W'] =  all_set['IS_HOLIDAY_WEEK'].shift(-nr_horizon)\n",
    "                all_set[f'IS_4W_BF_HOLIDAY_{nr_horizon}W'] =  all_set['IS_4W_BF_HOLIDAY'].shift(-nr_horizon)\n",
    "                all_set[f'IS_3W_BF_HOLIDAY_{nr_horizon}W'] =  all_set['IS_3W_BF_HOLIDAY'].shift(-nr_horizon)\n",
    "                all_set[f'IS_2W_BF_HOLIDAY_{nr_horizon}W'] =  all_set['IS_2W_BF_HOLIDAY'].shift(-nr_horizon)\n",
    "                all_set[f'IS_1W_BF_HOLIDAY_{nr_horizon}W'] =  all_set['IS_1W_BF_HOLIDAY'].shift(-nr_horizon)\n",
    "                all_set[f'IS_5W_BF_HOLIDAY_{nr_horizon}W'] =  all_set['IS_5W_BF_HOLIDAY'].shift(-nr_horizon)\n",
    "                all_set[f'IS_6W_BF_HOLIDAY_{nr_horizon}W'] =  all_set['IS_6W_BF_HOLIDAY'].shift(-nr_horizon)\n",
    "                all_set[f'IS_7W_BF_HOLIDAY_{nr_horizon}W'] =  all_set['IS_7W_BF_HOLIDAY'].shift(-nr_horizon)\n",
    "                all_set[f'IS_8W_BF_HOLIDAY_{nr_horizon}W'] =  all_set['IS_8W_BF_HOLIDAY'].shift(-nr_horizon)\n",
    "                all_set[f'IS_9W_BF_HOLIDAY_{nr_horizon}W'] =  all_set['IS_9W_BF_HOLIDAY'].shift(-nr_horizon)\n",
    "                all_set[f'IS_10W_BF_HOLIDAY_{nr_horizon}W'] =  all_set['IS_10W_BF_HOLIDAY'].shift(-nr_horizon)\n",
    "#                     all_set[f'IS_11W_BF_HOLIDAY_{nr_horizon}W'] =  all_set['IS_11W_BF_HOLIDAY'].shift(-nr_horizon)\n",
    "#                     all_set[f'IS_12W_BF_HOLIDAY_{nr_horizon}W'] =  all_set['IS_12W_BF_HOLIDAY'].shift(-nr_horizon)\n",
    "\n",
    "                all_set[horizon_prediction] = all_set['WEEKLY_SO_ORDER'].shift(-nr_horizon).copy()\n",
    "\n",
    "                all_set = all_set.dropna()\n",
    "\n",
    "                if nr_horizon <=5 :\n",
    "\n",
    "                    SO_train_set = all_set.loc[(all_set['PERIOD_TAG'] < first_test_date), :]\n",
    "                    X_train = SO_train_set.drop(list(all_set.filter(regex='SI_'))+[horizon_prediction]+['END_WEEK_INVENTORY','PERIOD_TAG', 'STANDARD_SKU_DESC', 'PROVINCE_REGION','YEAR'], axis = 1).copy()\n",
    "                    y_train = SO_train_set[horizon_prediction].copy()\n",
    "                    print(X_train.columns)\n",
    "\n",
    "                    print('--------------------------------------------------------')\n",
    "\n",
    "                    ##### Sell Out Test ##list(all_set.filter(regex='SI_'))+\n",
    "\n",
    "                    SO_test_set = all_set.loc[(all_set['PERIOD_TAG'] >= first_test_date)  & \\\n",
    "                                                (all_set['PERIOD_TAG'] <= end_test_date ), :]\n",
    "                    X_test =  SO_test_set.drop(list(all_set.filter(regex='SI_'))+[horizon_prediction]+['END_WEEK_INVENTORY','PERIOD_TAG','STANDARD_SKU_DESC', 'PROVINCE_REGION','YEAR'], axis = 1).copy()\n",
    "                    y_test =  SO_test_set[horizon_prediction].copy()\n",
    "                else:\n",
    "                    historical_vars_window_5 = list(all_set.filter(regex='_1'))+list(all_set.filter(regex='_2'))+list(all_set.filter(regex='_3'))+list(all_set.filter(regex='_4'))+list(all_set.filter(regex='_5'))\n",
    "\n",
    "                    SO_train_set = all_set.loc[(all_set['PERIOD_TAG'] < first_test_date), :]\n",
    "                    X_train = SO_train_set.drop(list(all_set.filter(regex='SI_'))+ historical_vars_window_5 + [horizon_prediction]+['END_WEEK_INVENTORY','PERIOD_TAG','STANDARD_SKU_DESC', 'PROVINCE_REGION','YEAR'], axis = 1).copy()\n",
    "                    y_train = SO_train_set[horizon_prediction].copy()\n",
    "                    print(X_train.columns)\n",
    "\n",
    "                    print('--------------------------------------------------------')\n",
    "\n",
    "                    ##### Sell Out Test ##\n",
    "\n",
    "                    SO_test_set = all_set.loc[(all_set['PERIOD_TAG'] >= first_test_date)  & \\\n",
    "                                                (all_set['PERIOD_TAG'] <= end_test_date ), :]\n",
    "                    X_test =  SO_test_set.drop(list(all_set.filter(regex='SI_')) + historical_vars_window_5 + [horizon_prediction]+['END_WEEK_INVENTORY', 'PERIOD_TAG','STANDARD_SKU_DESC', 'PROVINCE_REGION','YEAR'], axis = 1).copy()\n",
    "                    y_test =  SO_test_set[horizon_prediction].copy()\n",
    "\n",
    "\n",
    "                ################################################################ Add a forecast to a dataset ####################\n",
    "                ######## LightGBM ##########\n",
    "                param_grid = {\n",
    "                    'task': ['train'], \n",
    "                    'boosting_type': ['gbdt'],\n",
    "                    'objective': ['regression_l1'],\n",
    "                    'metric': ['rmse'],\n",
    "                    'n_estimators': [100],\n",
    "                    'max_depth': [-1],\n",
    "                    'feature_fraction': [0.1,0.9],\n",
    "                    'bagging_fraction': [0.7],\n",
    "                    'num_leaves': [10,50],\n",
    "                    'learning_rate': [0.05],\n",
    "                    'reg_alpha': [0.01, 0.1, 0.5]\n",
    "                }\n",
    "\n",
    "                param_candidates = ParameterGrid(param_grid)\n",
    "                print(f'{len(param_candidates)} candidates')\n",
    "\n",
    "                estimator = lgb.LGBMRegressor(metric='mape', \n",
    "                                              n_jobs=-1, \n",
    "                                              verbose=-1,\n",
    "                                              random_state=123, \n",
    "                                              num_boost_round = 300,\n",
    "                                              early_stopping_rounds = 20)\n",
    "                def fit_model(params):\n",
    "                    model1 = estimator.set_params(**params)\n",
    "\n",
    "                    model1.fit(X_train, y_train,eval_set=[(X_train, y_train)])\n",
    "                    score = model1.score(X_test, y_test)\n",
    "                    return [params, score]\n",
    "\n",
    "                results = Parallel(n_jobs = -1, backend = \"threading\")(delayed(fit_model)(params) for params in param_candidates)\n",
    "                print(max(results, key=lambda x: x[1]))\n",
    "\n",
    "                final_params = max(results, key=lambda x: x[1])[0]\n",
    "                R2 = max(results, key=lambda x: x[1])[1]\n",
    "\n",
    "\n",
    "                model = estimator.set_params(**final_params)\n",
    "\n",
    "                model.fit(X_train, y_train,eval_set=[(X_train, y_train)])\n",
    "\n",
    "                f_i = list(zip(X_train.columns.values,model.feature_importances_))\n",
    "                f_i.sort(key = lambda x : x[1])\n",
    "\n",
    "                final_features = [i[0] for i in f_i][-20:]\n",
    "\n",
    "                final_features_top = final_features_top + final_features\n",
    "\n",
    "        #         y_pred_train = model.predict(X_train)\n",
    "\n",
    "                SO_test_set[f'Pred_{horizon_prediction}'] =  model.predict(X_test)\n",
    "\n",
    "                SO_Pred[f'Pred_{horizon_prediction}'] = SO_test_set[f'Pred_{horizon_prediction}'].copy()\n",
    "\n",
    "                SO_Pred[f'{horizon_prediction}'] =  SO_test_set[horizon_prediction].copy()\n",
    "\n",
    "\n",
    "                print(f\"Forecast Accuracy RF MAPE: {mean_absolute_percentage_error(y_test, SO_test_set[f'Pred_RF_{horizon_prediction}'])}\")\n",
    "\n",
    "\n",
    "                plt.figure(figsize=(25,6))\n",
    "                plt.plot(range(0,len(y_test)),y_test, color = 'red', linewidth=2.0, alpha = 0.6)\n",
    "                plt.plot(range(0,len(y_test)), SO_test_set[f'Pred_RF_{horizon_prediction}'], color = 'blue', linewidth=0.8)\n",
    "                plt.legend(['Actual SO','Predicted SO' ])\n",
    "                plt.xlabel('Timestamp')\n",
    "                plt.title(f\"{i} - {j} - Actual Sell Out vs Predicted Sell out\")\n",
    "                plt.show()\n",
    "\n",
    "            SO_test = all_set.loc[(all_set['PERIOD_TAG'] >= first_test_date)  & \\\n",
    "                                        (all_set['PERIOD_TAG'] <= end_test_date ), :].drop(['SO_13W'],axis = 1)\n",
    "\n",
    "            SO_test = pd.concat([SO_test['STANDARD_SKU_DESC'],SO_Pred], axis = 1)\n",
    "            SO_test_result = pd.concat([SO_test_result, SO_test])\n",
    "            SO_Pred = pd.DataFrame()\n",
    "\n",
    "        elif ((sum(data_set[data_set['YEAR']==2023]['WEEKLY_SO_ORDER']) > 0) \n",
    "            & (data_set[(data_set['YEAR']>=2022)&(data_set['PERIOD_TAG'] < first_test_date)].shape[0] < 52)\n",
    "             & (data_set[(data_set['YEAR']>=2022)&(data_set['PERIOD_TAG'] < first_test_date)].shape[0] > 8)):\n",
    "    # & (data_set[(data_set['YEAR']>=2022)&(data_set['PERIOD_TAG'] < first_test_date)].shape[0] > 13)                          \n",
    "            ETS_first_test_date = dt.datetime.strptime('2023-08-31', '%Y-%m-%d')\n",
    "\n",
    "\n",
    "            data_set = data_set.set_index(data_set['PERIOD_TAG'])\n",
    "            data_set.index = pd.to_datetime(data_set.index)\n",
    "\n",
    "            SO_train_set = data_set[data_set['PERIOD_TAG'] < ETS_first_test_date]\n",
    "            SO_train = pd.Series(SO_train_set['WEEKLY_SO_ORDER'])\n",
    "\n",
    "            SO_test_set = data_set[(data_set['PERIOD_TAG'] >= ETS_first_test_date)&(data_set['PERIOD_TAG'] <= ETS_first_test_date+dt.timedelta(weeks=13))]\n",
    "            SO_test = pd.Series(SO_test_set['WEEKLY_SO_ORDER'])\n",
    "            \n",
    "#             index = pd.date_range(\"2023-03-20\", \"2023-07-31\", freq='W-MON')\n",
    "#             SO_train_1 = pd.Series(SO_train, index=index)\n",
    "            # fit in statsmodels\n",
    "            from statsmodels.tsa.exponential_smoothing.ets import ETSModel\n",
    "            model = ETSModel(SO_train, \n",
    "                seasonal=\"add\",\n",
    "                seasonal_periods=4,\n",
    "                initial_level=SO_train.mean(), \n",
    "                            )\n",
    "            fit = model.fit()\n",
    "\n",
    "            SO_pred = fit.predict(start = '2023-08-31', end = '2023-10-30')\n",
    "            SO_test_set['SO_ETS_pred'] = SO_pred\n",
    "            SO_test_result_ETS = pd.concat([SO_test_result_ETS,SO_test_set[['STANDARD_SKU_DESC','SO_ETS_pred','WEEKLY_SO_ORDER']]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89991a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for horizon_prediction in target_columns[0:13]:\n",
    "    SO_test_result[f'Actual_Pred_Diff_{horizon_prediction}'] = (SO_test_result[horizon_prediction]-SO_test_result[f'Pred_RF_{horizon_prediction}']).abs()/SO_test_result[horizon_prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f09c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "SO_test_result.to_csv('Test_v2101_20sku.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6715e763",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
